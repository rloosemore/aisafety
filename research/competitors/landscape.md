# AI Safety Competitive Landscape

*Research compiled February 2026*

---

## 1. Newsletters & Digests

### Import AI (Jack Clark)
- **URL:** https://importai.substack.com/
- **What it provides:** Weekly newsletter covering AI research, applications, and policy. Deep analysis of cutting-edge papers and industry developments.
- **Target audience:** AI researchers, engineers, policymakers, and informed generalists
- **Pricing:** Free tier + paid at $10/month or $100/year (early access to special essays/analysis)
- **Subscribers:** 104,000+
- **Strengths:** Written by Anthropic co-founder; deep technical credibility; massive reach; covers safety alongside capabilities
- **Weaknesses:** Broad AI focus (not safety-specific); high-level analysis may not satisfy deep technical readers; one voice/perspective

### Center for AI Safety (CAIS) Newsletter
- **URL:** https://newsletter.safe.ai/
- **What it provides:** Regular updates on AI safety research, policy developments, and CAIS activities
- **Target audience:** AI safety researchers, policy professionals, and engaged public
- **Pricing:** Free
- **Strengths:** Institutional credibility; safety-focused; connects research to policy
- **Weaknesses:** Organizational newsletter (may feel promotional); less independent commentary

### Don't Worry About the Vase (Zvi Mowshowitz)
- **URL:** https://thezvi.substack.com/
- **What it provides:** Weekly AI roundups with detailed commentary, plus broader rationalist/policy analysis
- **Target audience:** Rationalist community, AI safety enthusiasts, policy-interested readers
- **Pricing:** Free tier + paid subscription (estimated ~$10/month based on Substack norms)
- **Subscribers:** 31,000+
- **Strengths:** Extremely thorough weekly roundups; strong independent voice; covers safety concerns prominently; good at synthesizing many sources
- **Weaknesses:** Very long posts (can overwhelm); rationalist framing may alienate some readers; one person's opinions

### AI Safety & Governance Newsletter (Victor Akinwande)
- **URL:** https://www.safeai.news/
- **What it provides:** Highlights emerging topics and news on AI safety and governance worldwide
- **Target audience:** Safety and governance professionals
- **Pricing:** Free (Substack)
- **Strengths:** Governance-focused; global perspective
- **Weaknesses:** Smaller audience; less established brand

### Alignment Newsletter (Rohin Shah) -- INACTIVE
- **URL:** https://rohinshah.com/alignment-newsletter/
- **What it provides:** Was a weekly summary of AI alignment research with expert opinions
- **Target audience:** Alignment researchers and technically-inclined readers
- **Pricing:** Free
- **Status:** On indefinite hiatus (Rohin Shah now at Google DeepMind)
- **Strengths:** Was the gold standard for alignment research summaries
- **Weaknesses:** No longer active -- leaves a significant gap in the ecosystem

### AI Safety Events & Training Newsletter
- **URL:** https://aisafetyeventsandtraining.substack.com/
- **What it provides:** Weekly updates on AI safety events, workshops, training opportunities
- **Target audience:** People looking to attend events or find training in AI safety
- **Pricing:** Free
- **Strengths:** Unique aggregation of events; practical/actionable
- **Weaknesses:** Narrow focus on events only

---

## 2. Educational Programs

### BlueDot Impact (formerly AI Safety Fundamentals)
- **URL:** https://bluedot.org/
- **What it provides:** Free cohort-based courses on technical AI safety, AI governance, and future of AI. Certificate on completion. Multiple tracks available.
- **Target audience:** Early-career professionals and students looking to enter AI safety
- **Pricing:** Free
- **Alumni:** 7,000+ trained since 2022
- **Strengths:** Primary entry point into AI safety field; alumni at Anthropic, OpenAI, UK AISI; strong community; multiple course tracks; certificates
- **Weaknesses:** Competitive admissions; cohort-based (not self-paced); limited advanced content; some criticism of being too introductory for experienced professionals

### MATS (ML Alignment & Theory Scholars)
- **URL:** https://www.matsprogram.org/
- **What it provides:** 12-week in-person research program connecting talented researchers with top mentors. Summer 2026 will be the largest cohort (120 fellows, 100 mentors).
- **Target audience:** Aspiring alignment researchers (graduate-level)
- **Pricing:** Free (funded fellowships with stipends)
- **Location:** Berkeley and London
- **Strengths:** Top-tier mentorship (Anthropic, Redwood, ARC, UK AISI); research output; career launch pad; growing rapidly
- **Weaknesses:** Highly selective; requires relocating; not accessible to mid-career professionals or those who can't take 12 weeks off

### SPAR (Search for Pragmatic AI Risks)
- **URL:** https://sparai.org/
- **What it provides:** Part-time, remote research fellowship. 3 months culminating in Demo Day.
- **Target audience:** Aspiring AI safety and policy researchers
- **Pricing:** Free
- **Strengths:** Remote and part-time (more accessible); practical research projects; connections to METR, Redwood, GovAI, MATS
- **Weaknesses:** Less intensive than MATS; newer program

### AI Safety Collab
- **What it provides:** Community-driven AI safety learning experiences, expanding on BlueDot's model
- **Target audience:** Broader audience including those not accepted to BlueDot
- **Pricing:** Free
- **Strengths:** More accessible; community-organized; flexible
- **Weaknesses:** Variable quality across local groups; less structured

### AI Safety Camp
- **URL:** https://www.aisafety.camp/
- **What it provides:** Multi-week virtual and in-person research collaboration programs
- **Target audience:** People looking to do hands-on AI safety research projects
- **Pricing:** Free
- **Strengths:** Project-based learning; team collaboration; concrete outputs
- **Weaknesses:** Time-intensive; virtual format can be isolating

### AISafety.com Self-Study Curricula
- **URL:** https://www.aisafety.com/self-study
- **What it provides:** Curated self-study paths for AI safety education
- **Target audience:** Self-directed learners at all levels
- **Pricing:** Free
- **Strengths:** Self-paced; accessible to anyone; no application
- **Weaknesses:** No community; no mentorship; no certificate; requires high self-motivation

### AI Safety, Ethics, and Society Virtual Course
- **URL:** https://www.aisafetybook.com/virtual-course
- **What it provides:** Free virtual course on AI safety, ethics, and societal implications
- **Target audience:** General audience, students
- **Pricing:** Free
- **Strengths:** Broad coverage of ethics and society; accessible
- **Weaknesses:** Less technical depth

### Anthropic Fellows Program
- **URL:** https://alignment.anthropic.com/
- **What it provides:** Research fellowship for AI safety at Anthropic (applications for May & July 2026)
- **Target audience:** Researchers interested in alignment research
- **Pricing:** Funded fellowship
- **Strengths:** Work directly at leading safety lab; high prestige
- **Weaknesses:** Extremely selective; organization-specific

### LASR Labs
- **URL:** https://www.lasrlabs.org/
- **What it provides:** AI safety research program
- **Target audience:** Aspiring AI safety researchers
- **Pricing:** Free
- **Strengths:** Additional pathway into AI safety research
- **Weaknesses:** Smaller and less established

### CSA Trusted AI Safety Expert (TAISE) Certificate
- **URL:** https://cloudsecurityalliance.org/education/taise
- **What it provides:** Professional certification in AI safety, governance, and security (built with Northeastern University)
- **Target audience:** IT/security professionals, compliance officers
- **Pricing:** Exam ~$599; training $799-$2,500
- **Strengths:** Professional credential; recognized by industry; practical governance focus
- **Weaknesses:** Enterprise/compliance-focused (not alignment-focused); expensive

### Horizon Institute AI Policy Workshop
- **URL:** https://horizonpublicservice.org/ai-innovation-security-policy-workshop/
- **What it provides:** 3-day workshop in Washington DC on AI policy
- **Target audience:** Aspiring policymakers
- **Pricing:** Free (travel covered for selected participants)
- **Strengths:** DC-based networking; hands-on policy work; fully funded
- **Weaknesses:** Very limited spots; US-centric

---

## 3. Community Platforms

### LessWrong
- **URL:** https://www.lesswrong.com/
- **What it provides:** Community blog/forum for rationalist thinking, heavily featuring AI safety/alignment content
- **Target audience:** Rationalist community, AI safety researchers, intellectually curious generalists
- **Pricing:** Free
- **Strengths:** Deep intellectual content; karma system surfaces quality; strong community norms; foundational texts of AI safety live here
- **Weaknesses:** High barrier to entry for newcomers; rationalist culture can be insular; information is scattered across many posts; intimidating to outsiders

### Alignment Forum
- **URL:** https://www.alignmentforum.org/
- **What it provides:** Curated subset of LessWrong focused specifically on AI alignment research
- **Target audience:** AI alignment researchers (technical)
- **Pricing:** Free
- **Strengths:** Highest-quality alignment-specific content; serious research discussions; curated
- **Weaknesses:** Very technical; small community; publishing there requires invitation/reputation; not welcoming to beginners

### EA Forum
- **URL:** https://forum.effectivealtruism.org/
- **What it provides:** Community forum for effective altruism, with significant AI safety and governance discussion
- **Target audience:** EA community, AI governance researchers, people exploring AI safety careers
- **Pricing:** Free
- **Strengths:** Broad coverage including governance and careers; community-driven; good for career advice and project ideas
- **Weaknesses:** EA brand somewhat tarnished; mixes AI safety with many other cause areas; culture can feel exclusive

### Rob Miles' Discord
- **What it provides:** Community discussion around AI safety, connected to Rob Miles' YouTube channel
- **Target audience:** Viewers of Rob Miles' videos; AI safety curious newcomers
- **Pricing:** Free
- **Strengths:** Welcoming to beginners; connected to popular YouTube content; Stampy chatbot for Q&A
- **Weaknesses:** Discord-based (ephemeral; hard to search); quality varies

### EleutherAI Discord
- **What it provides:** Open-source AI research community with safety-relevant discussions
- **Target audience:** ML researchers, open-source AI enthusiasts
- **Pricing:** Free
- **Strengths:** Technical depth; active community; hands-on research
- **Weaknesses:** Not safety-focused per se; very technical

### AI Safety Slack / Alignment Ecosystem Slack
- **What it provides:** Invite-only Slack workspaces for AI safety professionals
- **Target audience:** Active AI safety researchers and professionals
- **Pricing:** Free (invite required)
- **Strengths:** Professional networking; real-time discussion; trusted community
- **Weaknesses:** Invite-only (gatekeeping); information stays siloed; not searchable publicly

### Reddit Communities
- **Subreddits:** r/ControlProblem, r/AIAlignment, r/ArtificialIntelligence (AI safety threads)
- **Target audience:** General public interested in AI safety
- **Pricing:** Free
- **Strengths:** Low barrier to entry; large reach potential; anonymous participation
- **Weaknesses:** Lower signal-to-noise ratio; less expert participation; Reddit culture

---

## 4. Career Resources

### 80,000 Hours
- **URL:** https://80000hours.org/
- **What it provides:** Comprehensive career advice for high-impact careers, with extensive AI safety coverage. Includes career reviews, problem profiles, job board (800+ roles), AI safety reading list, upskilling resources (67+ resources), and survey data on what AI safety orgs want in hires.
- **Target audience:** Early to mid-career professionals considering high-impact careers
- **Pricing:** Free (all content and 1-on-1 advising)
- **Strengths:** Most comprehensive AI safety career resource; regularly updated; job board; 1-on-1 advising; strong brand; research-backed
- **Weaknesses:** EA-affiliated (some audience skepticism); career advice skews toward young professionals; can feel prescriptive; limited help for senior professionals

### AI Safety Career Guide (aisafety.com)
- **URL:** https://www.aisafety.com/
- **What it provides:** Curated listings of events, communities, courses, and career resources
- **Target audience:** People exploring AI safety as a career
- **Pricing:** Free
- **Strengths:** Comprehensive directory; regularly updated
- **Weaknesses:** Aggregator rather than original analysis

---

## 5. Research Aggregators & Indexes

### Future of Life Institute AI Safety Index
- **URL:** https://futureoflife.org/ai-safety-index-summer-2025/
- **What it provides:** Evaluates 7 leading AI companies on 33 indicators across 6 domains. Published biannually (Summer 2025, Winter 2025 editions).
- **Target audience:** Policymakers, journalists, researchers, advocacy organizations
- **Pricing:** Free
- **Strengths:** Systematic company comparison; high visibility; policy-relevant
- **Weaknesses:** Focuses on company behavior (not research content); limited to 7 companies

### International AI Safety Report
- **URL:** https://internationalaisafetyreport.org/
- **What it provides:** Internationally shared, science-based assessment of AI capabilities, risks, and risk management. Chaired by Yoshua Bengio with 100+ experts. 2026 edition published.
- **Target audience:** Policymakers, researchers, international organizations
- **Pricing:** Free
- **Strengths:** Authoritative; international scope; comprehensive; expert-driven
- **Weaknesses:** Infrequent updates; report format (not a dynamic tool); policy-focused

### OECD.AI Policy Observatory
- **URL:** https://oecd.ai/
- **What it provides:** Access to 900+ national AI policies and initiatives, live data, and policy blog
- **Target audience:** Policymakers, governance researchers
- **Pricing:** Free
- **Strengths:** Largest collection of AI policy data globally; authoritative source; live data
- **Weaknesses:** Policy-focused (not safety research); institutional pace of updates

### IAPP Global AI Law and Policy Tracker
- **URL:** https://iapp.org/resources/article/global-ai-legislation-tracker
- **What it provides:** Tracks AI legislative and policy developments across jurisdictions. Updated regularly (last: Feb 2026).
- **Target audience:** Legal professionals, compliance officers, policymakers
- **Pricing:** Free tracker; IAPP membership $50-$295/year for full access
- **Strengths:** Comprehensive legal tracking; regularly updated; professional quality
- **Weaknesses:** Legal/compliance focus; full access requires membership

### arXiv / Semantic Scholar / Google Scholar
- **What it provides:** Academic paper repositories with AI safety papers
- **Target audience:** Researchers
- **Pricing:** Free
- **Strengths:** Comprehensive; searchable; citeable
- **Weaknesses:** No curation; overwhelming volume; no synthesis; requires expertise to navigate

---

## 6. Tools & Frameworks

### METR (Model Evaluation and Threat Research)
- **URL:** https://metr.org/
- **What it provides:** Evaluations of autonomous capabilities of frontier AI models. Open-source evaluation tools used by UK AISI. Partners with Anthropic and OpenAI.
- **Target audience:** AI labs, safety institutes, researchers
- **Pricing:** Free / open-source tools
- **Strengths:** Leading evaluation org; open-source; trusted by major labs; "Canary" project with RAND
- **Weaknesses:** Focused on frontier model evals (narrow scope); tools require significant technical expertise

### Apollo Research
- **URL:** https://www.apolloresearch.ai/
- **What it provides:** Research on detecting deceptive behaviors in AI. Model evaluations and interpretability research.
- **Target audience:** AI safety researchers, AI labs
- **Pricing:** Research outputs are free/public
- **Strengths:** Focused on critical problem (deception); novel research contributions
- **Weaknesses:** Narrow focus; early-stage

### Redwood Research
- **URL:** https://www.redwoodresearch.org/
- **What it provides:** AI safety research; advises AI companies and governments on risk assessment and mitigation
- **Target audience:** AI labs, policymakers, researchers
- **Pricing:** Research is public; advisory services not publicly priced
- **Strengths:** Advises Google DeepMind, Anthropic; high-quality research; practical impact
- **Weaknesses:** Small team; advisory work is private/not widely accessible

### UK AISI Inspect Framework
- **What it provides:** Open-source AI evaluation infrastructure
- **Target audience:** AI safety researchers, evaluation teams
- **Pricing:** Free / open-source
- **Strengths:** Government-backed; open-source; used internationally
- **Weaknesses:** Technical; requires significant expertise to use

### OpenAI Safety Evaluations Hub
- **URL:** https://openai.com/safety/evaluations-hub/
- **What it provides:** Safety evaluation results and methodology
- **Target audience:** Researchers, policymakers
- **Pricing:** Free
- **Strengths:** Transparency from a frontier lab; standardized evaluations
- **Weaknesses:** Self-evaluations (conflict of interest concerns); limited scope

### Enterprise AI Governance Platforms
Platforms like **Holistic AI**, **IBM watsonx.governance**, **OneTrust AI Governance**, **Credo AI**, **Fiddler AI**, and **Atlan** provide:
- AI inventory and risk management
- Compliance monitoring (EU AI Act, etc.)
- Bias detection, drift monitoring
- Policy enforcement and reporting

**Target audience:** Enterprise compliance/governance teams
**Pricing:** Enterprise SaaS (typically $50K-$500K+/year based on scale)
**Strengths:** Practical enterprise tools; compliance-driven; growing market (45.3% CAGR)
**Weaknesses:** Corporate compliance focus (not alignment/x-risk); expensive; not relevant to individual researchers

---

## 7. Paid Communities & Memberships

### IAPP (International Association of Privacy Professionals)
- **URL:** https://iapp.org/
- **What it provides:** Membership org for privacy and AI governance professionals. AIGP certification. Global tracker. Events.
- **Pricing:**
  - Student membership: $50/year
  - Government/nonprofit/education: $110/year
  - Professional: $295/year
  - AIGP exam: $649 (members) / $799 (non-members)
  - Certification maintenance: $250/2 years (waived for members)
- **Target audience:** Privacy and AI governance professionals
- **Strengths:** Professional credential; large network; growing AI governance focus
- **Weaknesses:** Privacy-first org (AI governance is newer addition); corporate/regulatory focus; not alignment-focused

### Future of Life Institute AI Existential Safety Community
- **URL:** https://futureoflife.org/
- **What it provides:** Community membership for researchers working on AI existential safety. Includes travel support, workshop invitations, networking.
- **Pricing:** Free (application/invitation required)
- **Strengths:** Prestigious network; travel funding; workshop access
- **Weaknesses:** Limited to established researchers; not a revenue model

### Cloud Security Alliance (CSA) AI Safety Initiative
- **URL:** https://cloudsecurityalliance.org/ai-safety-initiative
- **What it provides:** Working groups, research, TAISE certification
- **Pricing:** Free working group participation; paid certifications
- **Strengths:** Industry-recognized; practical focus
- **Weaknesses:** Security-focused (not alignment)

---

## 8. Policy Tracking Resources

(See also: IAPP Tracker, OECD.AI in Section 5)

### EU AI Act Tracking
- Multiple resources track compliance timelines (high-risk system rules effective August 2026)
- Fines up to EUR 35M or 7% of global turnover
- **Key implication:** Creating massive enterprise demand for governance tools and expertise

### National AI Safety Institutes
- **UK AI Security Institute** (renamed from AI Safety Institute in 2025)
- **US CAISI** (Center for AI Standards and Innovation, renamed from US AISI)
- **India AI Safety Institute** (announced January 2025)
- **International network** of AI Safety Institutes sharing resources
- These are government entities, not competitive offerings, but they shape the landscape significantly

---

## 9. Consulting & Advisory Services

### Frontier AI Lab Safety Teams
- Anthropic, OpenAI, Google DeepMind all have internal safety teams
- Not competitive offerings but they absorb much of the top talent

### Independent Safety Research Orgs
- **Redwood Research:** Advises AI companies and governments
- **METR:** Third-party evaluation services for AI labs
- **Apollo Research:** Deception-focused evaluations
- Pricing: Not publicly available; likely project-based contracts

### Enterprise AI Governance Consulting
- **Holistic AI:** Leading AI governance platform + consulting
- **Board of Innovation** and other management consultancies now offering AI safety/governance advisory
- **Pricing:** Enterprise rates ($200-$500+/hour for consulting; SaaS platform fees on top)
- **Target audience:** Large enterprises, particularly those affected by EU AI Act
- **Strengths:** Practical compliance help; growing market
- **Weaknesses:** Compliance-focused, not alignment/x-risk focused

### AI Risk Summit & Professional Events
- **URL:** https://www.airisksummit.com/
- **What it provides:** In-person conference for security/risk executives and AI researchers (Ritz-Carlton, Half Moon Bay)
- **Pricing:** Likely $1,000-$3,000+ per ticket (premium venue, executive audience)
- **Target audience:** CISOs, risk managers, AI researchers, policymakers
- **Strengths:** High-level networking; hands-on workshops; executive audience
- **Weaknesses:** Expensive; US-centric; corporate risk focus

---

## 10. Key Information & Resource Hubs

### AISafety.info (Stampy)
- **URL:** https://aisafety.info/
- **What it provides:** Interactive FAQ about AI existential risk. Hundreds of human-written answers plus AI-assisted chatbot for long-tail questions. Created by Rob Miles.
- **Target audience:** General public; newcomers to AI safety
- **Pricing:** Free (donation-supported via Manifund, Every.org)
- **Strengths:** Beginner-friendly; comprehensive FAQ; chatbot for uncommon questions; well-maintained
- **Weaknesses:** Volunteer-driven; limited funding; not a community

### AISafety.com
- **URL:** https://www.aisafety.com/
- **What it provides:** Hub for AI existential safety. Curated listings of events, communities, courses, self-study curricula.
- **Target audience:** Anyone interested in AI safety
- **Pricing:** Free
- **Strengths:** Comprehensive directory; regularly updated; good starting point
- **Weaknesses:** Aggregator (limited original content); no community features

### Rob Miles YouTube
- **URL:** YouTube channel "Robert Miles"
- **What it provides:** AI safety explainer videos covering technical topics accessibly
- **Target audience:** General public, AI-curious audiences
- **Subscribers:** 145,000+
- **Pricing:** Free (YouTube)
- **Strengths:** Best video content on AI safety; makes technical topics accessible; large reach
- **Weaknesses:** Infrequent uploads; one person; YouTube algorithm dependency

---

## What's Clearly Missing or Underserved

### 1. Curated Research Digest (Post-Alignment Newsletter)
The Alignment Newsletter's hiatus left a major gap. No single resource provides weekly, expert-curated summaries of alignment research with opinions. Import AI is broad; Zvi is comprehensive but overwhelming. Nothing fills the "here's what matters this week in alignment research, explained clearly" niche.

### 2. Mid-Career Professional Onboarding
Most educational programs target early-career people (students, recent grads). Mid-career professionals (engineers, managers, policymakers with 10+ years experience) lack structured, time-efficient pathways. They can't take 12 weeks off for MATS and BlueDot feels too introductory.

### 3. Practical Implementation Guidance
There's a gap between "understand AI safety concepts" and "implement safety practices in your organization." Enterprise governance platforms are expensive and compliance-focused. Individual engineers and small teams have few practical resources.

### 4. Cross-Domain Synthesis
Information is fragmented across technical safety, governance, policy, ethics, and enterprise compliance. No resource synthesizes across all these domains for people who need to understand the full picture (e.g., a CTO who needs to understand both alignment research and EU AI Act compliance).

### 5. Structured Knowledge Base
EA Forum/LessWrong content is scattered across thousands of posts with no clear navigation path. AISafety.info helps for FAQ-style questions but doesn't provide structured, progressive learning paths through the research literature.

### 6. Policy Practitioner Tools
Policymakers need digested, actionable intelligence about AI capabilities and risks -- not research papers. The International AI Safety Report is excellent but infrequent. Day-to-day policy work lacks good supporting tools.

### 7. Community for Working Professionals
Most communities are either very technical (Alignment Forum), very casual (Reddit), or insular (invite-only Slacks). There's no professional community for people working on AI safety in industry, government, or civil society that bridges these gaps.

### 8. Non-English Resources
Almost all resources are in English. Growing international interest (India, EU, etc.) is underserved.

---

## What People Complain About

Based on EA Forum posts, LessWrong discussions, and community feedback:

1. **Information overload:** Too many sources, no curation, impossible to keep up
2. **Beginner disorientation:** Many subfields, no clear starting path, most resources assume background knowledge
3. **Scattered information:** Valuable knowledge spread across LessWrong posts, Twitter, personal blogs, Slacks -- poorly structured and hard to find
4. **Insider culture:** AI safety community can feel cliquish; newcomers report feeling excluded
5. **Gap between understanding and action:** "I've read the intro materials, now what?" -- limited pathways from education to contribution
6. **Stale resources:** Many reading lists and guides go unmaintained
7. **Technical vs. governance divide:** Few resources bridge both; people in one camp often don't understand the other
8. **Career advice mismatch:** 80,000 Hours advice skews young; limited guidance for career changers or senior professionals
9. **Quality inconsistency:** Community-generated content varies wildly in quality and accuracy

---

## Where Willingness to Pay Exists but Options Are Limited

1. **Professional development / certification:** IAPP's AIGP ($649-$799) shows professionals will pay for credentials. No alignment-specific professional credential exists.

2. **Curated intelligence / briefings:** Import AI's 104K subscribers with paid tier ($100/yr) proves market. Corporate and policy professionals would pay more for sector-specific, actionable intelligence.

3. **Executive education / workshops:** AI Risk Summit and similar events command premium pricing. Short, intensive programs for executives on AI safety are scarce.

4. **Enterprise training:** Companies deploying AI need safety training for engineering teams. Current options are either too academic or too compliance-focused.

5. **Consulting / advisory:** The gap between academic safety research and practical implementation creates demand for advisory services. Redwood/METR serve only the largest labs.

6. **Community access:** Professionals routinely pay for curated professional communities (e.g., $500-$2,000/yr for tech communities). No equivalent exists in AI safety.

7. **Policy intelligence:** Government and corporate policy teams pay for regulatory intelligence services. AI safety policy tracking is largely free but fragmented.

8. **Tooling for safety evaluations:** Enterprise teams need evaluation tools but METR/UK AISI tools require deep expertise. Productized, user-friendly safety evaluation tools are missing.
