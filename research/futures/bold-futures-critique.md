# Critique: Bold Ideas for AI-Positive Futures

> Stress-test by critic-agent. Every idea evaluated from the perspective of a skeptical, scared, recently-displaced worker. The question is not "is this theoretically sound?" but "would someone who just lost their job look at this and feel empowered, or roll their eyes?"
>
> **Post-critique update:** After dialogue with the ideas-agent, two Tier 3 ideas were moved to Tier 2 with reworked mechanisms: Community Wealth Trusts (local businesses + municipal levies instead of FAANG stock) and Data Cooperatives (vertical-specific co-ops for healthcare/creative data instead of general consumer data). The ideas-agent accepted all other critiques including the kills of Time Banking 2.0, AI Apprenticeships, and Automation Impact Scores.

---

## Idea-by-Idea Critique

### 1. The Compute Dividend

| Test | Verdict |
|------|---------|
| Believability | MIXED. The Alaska model is powerful framing. But "tax every GPU-hour" immediately triggers "who will enforce this when these companies own Congress?" The gap between "call your representative" and "national compute tax" is enormous. |
| Agency | LOW. You can call your rep. That's it. That's the same advice every policy proposal gives. For someone who needs rent money in 30 days, "call your representative" is insulting. |
| Novelty | LOW. This is basically UBI funded by AI taxes. Andrew Yang ran on this in 2020. The "compute" framing is new, but the core idea is well-worn. |
| Specificity | MEDIUM. The Alaska comparison is great. The mechanism is clear. But "quarterly dividend" — how much? $50? $5,000? Without a number, it's a bumper sticker. |
| Co-optation risk | HIGH. Tech companies will lobby for exemptions, carve-outs, and definitions of "commercial inference" that exclude most of what they do. This is exactly what happened with corporate tax reform. |
| Unintended consequences | The dividend could be trivially small ($200/quarter) while being used as political cover for zero other support. "You already get your compute dividend, why are you complaining?" |

**Bottom line:** Solid anchor policy, but it's a years-long legislative fight. Cannot be the first thing you show a newly-displaced worker.

---

### 2. Neighborhood AI Guilds

| Test | Verdict |
|------|---------|
| Believability | HIGH. Five people, a group chat, and $20/month. Anyone can picture this. |
| Agency | HIGH. You could literally start this next Tuesday. No permission needed. |
| Novelty | MEDIUM. This is essentially a tech-focused mutual aid group. Mutual aid isn't new, but the AI-specific framing is. |
| Specificity | HIGH. The examples (bookkeeper bot, grant writer, tutor) are concrete and useful. |
| Co-optation risk | LOW. Hard to co-opt something that small and local. |
| Unintended consequences | Risk of becoming a "play with AI tools" club rather than something that produces economic value. Without structure, guilds could fizzle after the novelty wears off. Also, the people most displaced by AI may not want to spend their evenings learning more AI tools — that's like asking a coal miner to join a fracking club. |

**Bottom line:** One of the strongest ideas because it passes the "do it this week" test. But needs a serious answer to the emotional objection: "Why should I help AI get better at replacing me?"

---

### 3. The Right to Your Own Replacement

| Test | Verdict |
|------|---------|
| Believability | MEDIUM. The concept resonates emotionally ("I built the knowledge this AI stole"). But "legal ownership of a percentage of the AI system's output value" is almost impossible to implement. How do you prove your expertise trained the system? How do you value "a percentage of the AI system's output"? This is litigation-bait. |
| Agency | LOW. This requires legislation. You can lobby for it, but you can't do it yourself. |
| Novelty | HIGH. This genuinely reframes the conversation. Most displacement ideas are about cushioning the fall. This one says you have an ownership claim on the thing that replaced you. That's philosophically interesting. |
| Specificity | LOW. "A percentage" — what percentage? "5 years" — why 5? The mechanism sounds precise but is actually vague. |
| Co-optation risk | HIGH. Companies will argue the AI was trained on public data, not your specific expertise. Legal battles will take decades. Companies will pay pennies to settle and call it compliance. |
| Unintended consequences | Could create perverse incentives: companies avoid hiring humans at all (use contractors, offshore) so they never trigger the ownership provision. Could also create a system where only workers at large companies benefit — the freelancer whose writing trained GPT gets nothing. |

**Bottom line:** Compelling as a framing device and conversation-starter. Not viable as near-term policy. The gap between the emotional appeal and the implementation reality is too wide.

---

### 4. Municipal AI Utilities

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. Municipal broadband is a real precedent. But running AI infrastructure is orders of magnitude more complex and expensive than running fiber. Most cities can't maintain their roads. |
| Agency | MEDIUM. You can show up at a city council meeting, yes. But you'll be proposing that a city with a $50M budget build AI infrastructure that costs $100M+. That's a hard sell. |
| Novelty | MEDIUM. "Public option for AI" has been floating in policy circles. The municipal framing is somewhat fresh. |
| Specificity | MEDIUM. The broadband analogy is helpful, but glosses over the massive technical and financial differences. |
| Co-optation risk | MEDIUM. Could become another underfunded public option that can't compete with private offerings. See: public housing. |
| Unintended consequences | Could create a two-tier system where the municipal AI is a joke and everyone who can afford it uses the corporate version. Tax dollars wasted on an inferior product. |

**Bottom line:** Interesting at the principle level, but the implementation gap is large. Works better as a long-term vision than a near-term action item. The broadband comparison oversimplifies drastically.

---

### 5. The 1,000-Hour Transition Guarantee

| Test | Verdict |
|------|---------|
| Believability | HIGH. This sounds like something that could exist. Germany's Kurzarbeit is a powerful precedent. A personal advisor, a stipend, and concrete pathways — that's what people actually need. |
| Agency | MEDIUM. Requires state legislation, but it's a tangible thing to campaign for. You can say "I want the 1,000-Hour Guarantee" and people know what you mean. |
| Novelty | MEDIUM-LOW. This is essentially a better severance/retraining package. The German framing helps, but the core idea is familiar. |
| Specificity | HIGH. 1,000 hours, personal advisor, living stipend, skill assessment, three pathways. Concrete. |
| Co-optation risk | MEDIUM. Could be watered down to "1,000 hours of access to an online learning portal" with no stipend and no advisor. That's just existing government retraining programs with a new name. |
| Unintended consequences | "Retraining" for what? If AI is automating entire categories of knowledge work, where are these people transitioning TO? The 1,000-hour guarantee doesn't answer the fundamental question: transition to what job, exactly? |

**Bottom line:** Strong as a concrete policy demand. The "transition to what?" question is the elephant in the room and must be addressed directly, not hand-waved.

---

### 6. Data Cooperatives

| Test | Verdict |
|------|---------|
| Believability | LOW-MEDIUM. The concept is sound. The agricultural co-op analogy is helpful. But in practice, your data is already gone. Facebook has it, Google has it, OpenAI trained on it. The horse has left the barn. Telling someone "join a data co-op" when their data has been harvested for a decade feels like locking the door after the burglary. |
| Agency | MEDIUM. You could join one. But very few exist, and the ones that do have negligible bargaining power. |
| Novelty | LOW. Data cooperatives have been discussed since at least 2018. They haven't gained traction for structural reasons, not because nobody thought of them. |
| Specificity | LOW. "The co-op negotiates on behalf of thousands." With whom? For what? How much revenue? What does the average member actually receive? |
| Co-optation risk | HIGH. Companies will simply refuse to negotiate, change their ToS, or use synthetic data. The power asymmetry is enormous. |
| Unintended consequences | Could give people a false sense of data ownership while actual power dynamics remain unchanged. "I joined a data co-op" becomes feel-good activism with no material impact. |

**Bottom line:** Theoretically sound, practically dead on arrival in 2026. The data has already been extracted. This idea needed to happen 10 years ago. Might work for very specific, high-value data categories (medical, creative) but not as a general solution.

---

### 7. Automation Impact Scores

| Test | Verdict |
|------|---------|
| Believability | MEDIUM. ESG scores exist, yes. But ask anyone what their favorite company's ESG score is. Nobody cares. ESG has become a greenwashing tool, not a driver of behavior change. The analogy actually undermines the idea. |
| Agency | LOW. You can... check a score before buying? Consumer boycotts based on scores have a terrible track record. |
| Novelty | LOW. This is ESG for AI. ESG is widely regarded as a failure at driving actual corporate behavior change. |
| Specificity | MEDIUM. The scorecard concept is clear. But who runs it? Who audits? Who enforces? |
| Co-optation risk | VERY HIGH. This is ESG's exact failure mode. Companies will game the score, hire consultants to optimize their rating, and use a good score as PR while continuing to automate aggressively. "Automation-washing." |
| Unintended consequences | Could legitimize aggressive automation. "Our Automation Impact Score is B+, so our mass layoffs were responsible." |

**Bottom line:** The ESG comparison should be a warning, not an inspiration. This has already been tried in a different domain and it hasn't worked. Needs a fundamentally different enforcement mechanism to avoid the same fate.

---

### 8. AI Apprenticeships (Human-AI Team Model)

| Test | Verdict |
|------|---------|
| Believability | MEDIUM. The "power drill" analogy is effective. But the mandate of a 2-year transition period is unrealistic. Companies are automating precisely because human labor costs more. Forcing them to keep paying humans for 2 years to "supervise" the AI defeats the economic purpose of automation. Companies will find ways around it. |
| Agency | LOW. Requires legislation mandating transition periods. |
| Novelty | LOW-MEDIUM. "Workers supervise the AI" has been the standard corporate talking point for years. "Your job won't disappear, it will evolve!" This is what every CEO says before the layoffs. |
| Specificity | MEDIUM. The 2-year timeline is specific. But what happens at month 25? You get laid off anyway? |
| Co-optation risk | VERY HIGH. This is already the corporate narrative: "We're not replacing workers, we're augmenting them!" It has been used as PR cover for gradual workforce reduction for years. |
| Unintended consequences | Creates a false sense of security. Workers spend 2 years "supervising" the AI, then get laid off when the mandate expires. They've lost 2 years they could have spent genuinely transitioning. Worse: their "AI supervisor" skills may not transfer. |

**Bottom line:** This is the most dangerous idea in the set because it sounds reasonable but is actually the corporate playbook for managed decline. The "power drill" analogy is misleading — power drills didn't eliminate construction workers, but AI is eliminating entire job categories. The analogy breaks at exactly the point where it matters.

---

### 9. The Displacement Insurance Pool

| Test | Verdict |
|------|---------|
| Believability | HIGH. Insurance is something everyone understands. 80% of salary for 18 months is concrete and meaningful. |
| Agency | MEDIUM. Requires legislation, but it's a concrete policy demand. |
| Novelty | LOW. This is unemployment insurance with a new name. The AI-specific branding adds little. |
| Specificity | HIGH. 80% salary, 18 months, funded by employer/worker contributions. Clear mechanism. |
| Co-optation risk | MEDIUM. Could be set at inadequate levels. Companies could lobby for low employer contributions. |
| Unintended consequences | 18 months runs out. Then what? If the displacement is permanent (which it increasingly is), insurance just delays the crisis. Also, employer contribution requirements could incentivize companies to use contractors and gig workers who don't trigger the requirement. |

**Bottom line:** Solid incremental policy, but it's a band-aid on a structural wound. If AI displacement is permanent rather than transitional, insurance that assumes you'll find a new job in 18 months is based on a false premise.

---

### 10. Community Wealth Trusts

| Test | Verdict |
|------|---------|
| Believability | MEDIUM. Community land trusts are real. But "own shares in AI companies" is a fantasy for most communities. What shares? Bought with what money? Nvidia stock is $800/share. A neighborhood trust buying meaningful equity in AI companies is not realistic. |
| Agency | LOW-MEDIUM. You can advocate for a trust, but creating one that actually holds meaningful assets requires significant capital and legal infrastructure. |
| Novelty | MEDIUM. Applying the trust model to AI wealth is a somewhat fresh angle. |
| Specificity | LOW. "The trust owns shares in AI companies." Which ones? Bought how? With whose money? The community land trust analogy breaks down because land is local and purchasable; AI company equity is global and expensive. |
| Co-optation risk | MEDIUM. The trust could hold trivial amounts of stock while being touted as "community ownership." |
| Unintended consequences | Creates false sense of community ownership. A trust that owns $50,000 in AI stocks is not "community wealth" — it's a symbol. |

**Bottom line:** The concept of community ownership of AI wealth is right. The mechanism proposed is not credible. Needs a much more realistic path to meaningful asset accumulation.

---

### 11. The AI Licensing Exam

| Test | Verdict |
|------|---------|
| Believability | HIGH. Licensing is intuitive. "If a doctor needs a license, why doesn't the AI that replaces the doctor?" is a powerful rhetorical question. |
| Agency | MEDIUM. You can lobby state licensing boards. Concrete target. |
| Novelty | MEDIUM. AI regulation is a hot topic, but the licensing-exam framing (as opposed to "regulation" in general) is somewhat fresh. |
| Specificity | HIGH. Specific domains listed, clear mechanism, existing institutional infrastructure (licensing boards) to build on. |
| Co-optation risk | MEDIUM-HIGH. Tech companies will capture the licensing process, write the standards, and ensure their systems always pass. This is regulatory capture 101. |
| Unintended consequences | Could create a barrier that only large companies can afford to pass, entrenching their dominance. Small open-source AI projects can't afford licensing compliance. Could slow beneficial AI applications in healthcare and education. |

**Bottom line:** Good idea with genuine traction potential. Must be designed to resist regulatory capture. The concern about entrenching large players is real and needs to be addressed in the proposal.

---

### 12. Open-Source AI Commons

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. Linux and Wikipedia are powerful precedents. People understand "public" vs "corporate." |
| Agency | LOW-MEDIUM. You can use and contribute to open-source AI. You can advocate for public funding. But building competitive open-source AI requires billions. |
| Novelty | LOW. Open-source AI is already a movement (Meta's Llama, Mistral, etc.). This adds public funding, which is an incremental policy proposal, not a new idea. |
| Specificity | MEDIUM. "1-2% tax on commercial AI revenue" is specific. But who governs the foundation? How does it stay independent? |
| Co-optation risk | MEDIUM. "Open source" is already being co-opted by companies like Meta releasing models with restrictive licenses while calling them "open." |
| Unintended consequences | Publicly-funded AI could always lag behind corporate AI, creating a two-tier system. The foundation could be captured by former tech executives. "Open source" could be used as a cost-shifting strategy by companies that use it without contributing. |

**Bottom line:** Directionally correct, but "fund open-source AI" is not a new idea and the implementation challenges are massive. The PBS/BBC analogy is imperfect — public media has never competed with the scale of private media. Needs more thought on how to avoid becoming the also-ran.

---

### 13. Time Banking 2.0

| Test | Verdict |
|------|---------|
| Believability | LOW. "The things machines can't do become the economy's most valued resource." This is the kind of thing that sounds beautiful in a TED talk and absurd to someone who just lost a $75k/year job. "Your caregiving is now currency!" is not what they want to hear. It sounds like being told your poverty is actually a lifestyle choice. |
| Agency | MEDIUM. You can join a time bank. But time banks have existed for decades and have never become economically significant. |
| Novelty | LOW. Time banks are not new. Adding "AI-powered matching" is a marginal improvement. |
| Specificity | LOW-MEDIUM. What does "one hour of your human skill" actually get you? Can you pay rent with time credits? Buy groceries? If not, this is a hobby, not an economy. |
| Co-optation risk | LOW. Nobody with power cares enough about time banks to co-opt them. That's also the problem. |
| Unintended consequences | Romanticizes low-paid care work as "the new economy." This is exactly what displaced workers are afraid of — their $75k accounting job becoming "valued" at one time credit per hour, redeemable for someone else's equally devalued labor. |

**Bottom line:** This is the weakest idea in the set. It inadvertently proves the critic's worst fear: that the "AI-positive future" means formerly middle-class professionals trading babysitting hours because the real economy left them behind. Remove or radically rework.

---

### 14. The Automation Tax Rebate

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. Tax incentives are a proven policy tool. The mechanism is clear. |
| Agency | LOW. Legislative advocacy only. |
| Novelty | MEDIUM. The "flip" from taxing automation to rewarding transition is a useful reframing. |
| Specificity | HIGH. Trigger threshold, outcome tracking, existing policy precedents cited. |
| Co-optation risk | MEDIUM-HIGH. Companies will game the "successfully transitioned" metric. Transfer workers to shell subsidiaries, count them as "transitioned," claim the rebate. The Work Opportunity Tax Credit is already widely abused. |
| Unintended consequences | Could subsidize automation. "We get a tax break for transitioning workers, so let's automate faster and collect more rebates." Perverse incentive structure. |

**Bottom line:** Clever reframing, but the gaming risk is severe. If you can't measure "successful transition" rigorously, this becomes a subsidy for layoffs.

---

### 15. Citizen AI Assemblies

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. Ireland's citizens' assemblies are a genuine, powerful precedent. The "like jury duty" framing is immediately understandable. |
| Agency | MEDIUM. You can advocate for assemblies. The "you could be selected" angle gives genuine personal agency. |
| Novelty | MEDIUM-HIGH. Applying citizens' assemblies specifically to AI governance is a fresh and interesting application. |
| Specificity | HIGH. Concrete examples (school AI, hospital AI, policing AI). Clear mechanism (random selection, expert testimony, binding recommendations). |
| Co-optation risk | MEDIUM. Assemblies can be advisory rather than binding, neutering their power. Expert testimony can be dominated by industry. |
| Unintended consequences | Assemblies could make uninformed decisions if the expert testimony is biased or the deliberation process is flawed. Could slow beneficial AI deployment. Could be ignored by powerful interests. |

**Bottom line:** One of the strongest ideas. Democratic legitimacy is a powerful asset. The "you could be selected" angle is genuinely novel in the AI discourse. Needs strong protections against becoming merely advisory.

---

### 16. The "Made by Humans" Premium

| Test | Verdict |
|------|---------|
| Believability | MEDIUM. Fair Trade coffee is a real precedent. But Fair Trade has remained a niche market (2-5% of coffee sales). It didn't transform the industry; it created a premium segment for affluent consumers. |
| Agency | HIGH. You can buy "Made by Humans" products today (to the extent they exist). Consumer action is real and immediate. |
| Novelty | LOW-MEDIUM. "Buy human-made stuff" is already a sentiment. The certification angle adds structure but isn't transformative. |
| Specificity | MEDIUM. The certification concept is clear. But what counts as "primarily human labor" when AI assists almost everything? Where's the line? |
| Co-optation risk | HIGH. "Made by Humans" could become "humanwashing" the way "natural" became meaningless on food labels. Companies will claim human involvement in heavily automated processes. |
| Unintended consequences | Creates a luxury market for human labor that only wealthy consumers can access. Working-class people buy the AI-made version because it's cheaper. Human work becomes an artisanal luxury, not the norm. This could accelerate the two-tier economy rather than prevent it. |

**Bottom line:** Sounds empowering but could backfire. If "human-made" becomes a premium product, it means the default economy is fully automated and only rich people can afford the human touch. That's a dystopia wearing a fair-trade label.

---

### 17. Universal Transition Accounts

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. A career-transition version of a 401(k) is intuitive and reasonable. |
| Agency | LOW. Legislative advocacy. Cannot do this yourself. |
| Novelty | LOW. Singapore's SkillsFuture already exists. Individual training accounts have been proposed repeatedly. |
| Specificity | MEDIUM. Clear mechanism, but how much money? What's the contribution rate? How much would a typical worker have when they need it? |
| Co-optation risk | MEDIUM. Could be underfunded and become another inadequate program. |
| Unintended consequences | If AI displacement is fast and large-scale, individual accounts won't accumulate enough before they're needed. Someone who needs to transition in year 2 of their career hasn't saved enough. The people who need it most will have it least. |

**Bottom line:** Reasonable incremental policy but has the same problem as all savings-based solutions: the people most vulnerable to displacement are the ones with the least savings and the least time to accumulate.

---

### 18. AI Profit-Sharing Mandates

| Test | Verdict |
|------|---------|
| Believability | HIGH. Mexico and France already do this. The 15% threshold and 30% sharing rate are concrete. "If AI makes your company dramatically more profitable, you share the gains" is intuitively fair. |
| Agency | MEDIUM. Campaign for legislation. Concrete policy demand. |
| Novelty | MEDIUM. Applying existing profit-sharing law to AI-driven profits is a clear and specific proposal. |
| Specificity | HIGH. Clear thresholds, clear precedents, clear mechanism. |
| Co-optation risk | MEDIUM-HIGH. Companies will shift profits to subsidiaries, reclassify AI gains, or restructure to stay below the threshold. Creative accounting is a trillion-dollar industry. |
| Unintended consequences | Could incentivize companies to automate slowly (staying below 15%/year) rather than responsibly. Could be evaded through corporate restructuring. Could push companies to automate overseas where the mandate doesn't apply. |

**Bottom line:** One of the strongest policy proposals because it has real-world precedent and a clear mechanism. The evasion risk is real but manageable with good enforcement design. Worth making a centerpiece demand.

---

### 19. Sanctuary Sectors

| Test | Verdict |
|------|---------|
| Believability | HIGH. "We already require human pilots" is a devastating argument. Everyone understands the concept of "some things should have a human in charge." |
| Agency | MEDIUM. Campaign for specific sectors to be protected. Concrete, sector-by-sector advocacy. |
| Novelty | MEDIUM-HIGH. The "sanctuary" framing is powerful and emotionally resonant. Not just "regulation" but a statement about what society values. |
| Specificity | HIGH. Specific sectors named. Clear mechanism (human must be primary provider). Existing precedents. |
| Co-optation risk | MEDIUM. Companies will lobby to exclude their sector. "AI-assisted" will be defined so broadly that it means "AI does everything, human signs off." |
| Unintended consequences | Could create a labor ghetto. If sanctuary sectors are the only jobs left for humans, and they're historically underpaid (caregiving, teaching), you've created a future where humans are confined to the jobs AI doesn't want. Also: sanctuary sectors could become the dumping ground for all displaced workers, crashing wages in those sectors. |

**Bottom line:** Strong idea with real emotional and rhetorical power. The labor-ghetto risk must be addressed explicitly. Sanctuary sectors must include wage floors, not just employment requirements.

---

### 20. The Worker Transition Corps

| Test | Verdict |
|------|---------|
| Believability | MEDIUM-HIGH. AmeriCorps and CCC are real precedents. The concept of national service during crisis resonates. |
| Agency | MEDIUM. You could sign up or advocate for its creation. |
| Novelty | LOW-MEDIUM. This is AmeriCorps for AI. The concept is sound but not new. |
| Specificity | HIGH. 6-12 months, living stipend, health insurance, specific roles described. |
| Co-optation risk | LOW-MEDIUM. Government program, harder to co-opt than corporate-facing policies. |
| Unintended consequences | Could become a dead-end program. 12 months of "teaching digital literacy to seniors" does not create a career path. What happens after the Corps? |

**Bottom line:** Solid idea. The CCC analogy is powerful. But must answer the "what comes after?" question or it becomes a holding pen.

---

### 21-25. Bonus Ideas (Quick Assessment)

**21. AI Whistleblower Protections** — STRONG. Concrete, actionable, fills a real gap. Whistleblower protections work. Low co-optation risk. Should be elevated from bonus to main list.

**22. The Neighborhood Robot Tax** — WEAK. "Small monthly fee" to a neighborhood fund is trivially small. A local coffee shop paying $50/month for using AI scheduling software doesn't fund anything meaningful. Charming concept, negligible impact.

**23. Portable Benefits Backpack** — STRONG. Decoupling benefits from employment is one of the most important structural changes possible. This is undersold as a "bonus" idea. Should be a centerpiece. Not novel (gig economy advocates have pushed this for years), but critical.

**24. The AI Slowdown Clause** — MEDIUM. Good as a collective bargaining tool. But assumes unions exist in the affected sectors. Most knowledge workers being displaced are not unionized. Useful for sectors with existing unions, irrelevant for most white-collar workers. |

**25. Community AI Auditors** — MEDIUM. Accountability is important, but this is closer to a civic engagement idea than a displacement solution. Doesn't directly address the "I lost my job" problem.

---

## Tier Sorting

### Tier 1: "This could actually change the conversation"
These ideas are specific, credible, emotionally resonant, and could survive contact with a skeptical audience.

1. **#18 AI Profit-Sharing Mandates** — Real precedent (Mexico, France). Clear mechanism. Intuitively fair. Make this a headline demand.
2. **#15 Citizen AI Assemblies** — Novel application of proven democratic innovation. "You could be selected" gives real personal agency.
3. **#19 Sanctuary Sectors** — Powerful framing. "Human pilot" argument is devastating. Must include wage floors.
4. **#5 The 1,000-Hour Transition Guarantee** — Concrete and campaign-ready. Must address "transition to what?"
5. **#23 Portable Benefits Backpack** (elevated from bonus) — Structurally critical. Decoupling benefits from employment is a precondition for everything else.
6. **#21 AI Whistleblower Protections** (elevated from bonus) — Concrete, enforceable, fills a real gap.

### Tier 2: "Interesting but needs work"
Good ideas that need sharper framing, more realistic mechanisms, or better answers to obvious objections.

7. **#2 Neighborhood AI Guilds** — Great agency test score. Needs to answer "why should I help AI replace me?"
8. **#1 Compute Dividend** — Solid anchor policy but too long-term for a scared audience. Needs a near-term companion.
9. **#11 AI Licensing Exam** — Intuitive but must address regulatory capture risk.
10. **#12 Open-Source AI Commons** — Directionally right but not a new idea. Needs sharper framing.
11. **#9 Displacement Insurance Pool** — Solid but just better unemployment insurance. Needs differentiation.
12. **#20 Worker Transition Corps** — Good CCC analogy but must answer "what comes after?"
13. **#14 Automation Tax Rebate** — Clever reframing but severe gaming risk.
14. **#3 Right to Your Own Replacement** — Philosophically powerful but practically near-impossible to implement. Use as a framing device, not a policy proposal.
15. **#4 Municipal AI Utilities** — Interesting vision, unrealistic in the near term.
16. **#24 AI Slowdown Clause** — Good for unionized sectors, irrelevant for most displaced workers.

17. **#10 Community Wealth Trusts** — Reworked: trusts that own shares in LOCAL AI-enabled businesses (not FAANG), funded by municipal automation levies. The community land trust analogy holds at local scale. Needs the reworked mechanism to be credible.
18. **#6 Data Cooperatives** — Reworked: vertical-specific co-ops only (healthcare workers' data, musicians' training data). General consumer data ship has sailed, but scarce/regulated data verticals have real leverage.
19. **#24 AI Slowdown Clause** — Good for unionized sectors, irrelevant for most displaced workers.

### Tier 3: "Too vague or already tired"
These ideas either rehash familiar ground, fail the skeptical-audience test, or have fundamental structural problems.

20. **#13 Time Banking 2.0** — Accidentally describes a dystopia where middle-class workers trade care hours. Remove.
21. **#7 Automation Impact Scores** — ESG for AI, and ESG has failed. Humanwashing risk is enormous.
22. **#8 AI Apprenticeships** — This IS the corporate playbook for managed decline. Dangerous to endorse.
23. **#16 "Made by Humans" Premium** — Could accelerate the two-tier economy it claims to prevent.
24. **#17 Universal Transition Accounts** — The people who need it most will have it least.
25. **#22 Neighborhood Robot Tax** — Charming, negligible.
26. **#25 Community AI Auditors** — Civic engagement, not a displacement solution.

---

## Sharpening the Top Tier

**#18 AI Profit-Sharing Mandates** — Lead with the Mexico example. "Mexico has required companies to share 10% of profits with workers since 1917. Your company just tripled its profits using AI. Where's your share?" That's a message that lands.

**#15 Citizen AI Assemblies** — Frame as "AI Jury Duty." Everyone understands jury duty. "You could be called to serve on your city's AI Assembly. You would decide whether the school district uses AI grading. That's real power." Make people feel the agency.

**#19 Sanctuary Sectors** — Lead with the pilot analogy, then immediately address wage floors. "We require human pilots. We should require human teachers, nurses, and caregivers — and pay them accordingly." The wage floor is what makes this a jobs program, not a labor ghetto.

**#5 The 1,000-Hour Transition Guarantee** — Pair with a specific answer to "transition to what?" Sanctuary sectors, public service, community roles, small business support. The guarantee + the destination.

---

## Critical Gaps

The brainstorm is missing several important categories:

### Gap 1: Immediate survival
Almost none of these ideas help someone who lost their job THIS MONTH. Where's the emergency response? Mutual aid networks? Emergency UBI pilots? Displacement is happening NOW, not in a legislative cycle.

### Gap 2: Collective bargaining and labor power
Only one idea (#24) mentions unions. For most of labor history, workers gained power through organizing, not through government programs. Where are the ideas for new forms of collective bargaining adapted to the AI age? Cross-employer worker coalitions? Industry-wide AI negotiation?

### Gap 3: Corporate accountability with teeth
Most ideas here use carrots (tax rebates, scores, certifications). Where are the sticks? What about personal liability for executives who automate without transition support? Criminal penalties? Clawback provisions on executive compensation during mass AI layoffs?

### Gap 4: International coordination
AI displacement is global. A company can evade every US policy by automating overseas. Where are the ideas for international coordination — trade agreements with labor protections, cross-border worker solidarity, AI governance treaties?

### Gap 5: Psychological and identity support
Losing your job to AI is not just an economic crisis — it's an identity crisis. "I was an accountant for 20 years. Now I'm nothing." Where are the ideas that address the meaning and identity dimensions? Peer support networks, community roles, new forms of status and purpose?

---

## 5 Additional Ideas to Fill the Gaps

### A. The AI Displacement Emergency Fund (fills Gap 1)
Not insurance, not a transition account — an EMERGENCY fund. When mass AI layoffs hit a community (100+ jobs in a county within 90 days), the fund immediately activates: direct cash payments within 2 weeks, no applications, no means-testing. Funded by a small surcharge on AI company revenue. Modeled on disaster relief (FEMA), not welfare programs. The message: AI displacement IS a disaster, and we respond to disasters.

### B. The AI Workers' Alliance (fills Gap 2)
A new kind of labor organization — not tied to a single employer, but to a class of workers affected by AI across industries. Accountants, paralegals, customer service reps, translators, writers — all in one cross-industry alliance. The alliance negotiates with AI companies and governments on behalf of all displaced knowledge workers. Think of it as a union for the AI age: portable, cross-employer, and built for a labor market where your employer changes but the threat doesn't.

### C. Executive Clawback Provisions (fills Gap 3)
When a company conducts AI-driven mass layoffs (more than 500 workers in 12 months) while executive compensation increases, the top 5 executives' bonuses and stock grants for that year are clawed back into a worker transition fund. Not a tax, not a fine — a clawback of the specific money that went to the people who made the decision. Makes it personal. Makes the cost of reckless automation fall on the people who chose it.

### D. The AI Displacement Peer Network (fills Gap 5)
A structured peer support program connecting people who've lost jobs to AI — not a job-training program, a human connection program. Groups of 8-12 people meeting weekly, facilitated by trained peers (not therapists, not coaches — people who've been through it). Modeled on veteran peer support programs that have strong evidence of effectiveness. Addresses the identity crisis, not just the economic crisis. "You are not alone, you are not broken, and your experience matters."

### E. Trade Agreement Labor Clauses for AI (fills Gap 4)
Embed AI displacement protections into trade agreements. If a country wants preferential trade access, it must maintain minimum AI displacement protections (transition support, profit-sharing, worker consultation). Modeled on labor and environmental provisions in modern trade agreements. Prevents the "automate offshore to avoid regulations" loophole. Creates a floor below which no country can go.

---

## Summary Verdict

The brainstorm is a strong starting point with real depth. Its greatest strengths:
- Multiple scales of action (individual to structural)
- Real-world precedents for most ideas
- Some genuinely powerful framings (Sanctuary Sectors, AI Jury Duty, profit-sharing)

Its greatest weaknesses:
- Too many ideas that assume legislative timelines when people need help NOW
- Several ideas that sound good but are actually corporate-friendly (AI Apprenticeships, Made by Humans)
- Missing collective bargaining, emergency response, accountability with teeth, and identity/meaning support
- Some ideas are familiar policy proposals in new packaging (displacement insurance = unemployment insurance, transition accounts = SkillsFuture)

The audience test: a 45-year-old who just lost their accounting job would find maybe 5-6 of these 25 ideas genuinely compelling. The rest would feel too abstract, too distant, or too familiar. That's not a bad ratio for a brainstorm, but it means heavy editing is needed before these are published.
