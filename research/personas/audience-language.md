# Audience Language Research

> How different audience segments talk about AI safety, what they search for, and what language resonates.

---

## 1. Search Behavior Patterns

### Primary Search Terms (by volume and intent)

| Term | Who searches it | Intent |
|------|----------------|--------|
| "AI safety" | Broadest group: students, journalists, professionals, researchers | Orientation, overview |
| "AI alignment" | Technical community, career-changers, LessWrong-adjacent | Deep dive, research focus |
| "AI risk" | General public, policy audience, media consumers | Concern-driven, news-driven |
| "AI ethics" | Corporate, academic, policy, social justice communities | Fairness, bias, governance |
| "AI regulation" / "AI governance" | Policy professionals, lawyers, compliance officers | Professional, regulatory |
| "responsible AI" | Enterprise, corporate, product teams | Implementation, compliance |
| "AI existential risk" / "x-risk" | EA community, rationalist community, deeply engaged | Philosophical, long-term |
| "AI security" | Cybersecurity professionals, enterprise | Technical, immediate threats |

### Question-Format Searches (high intent, orientation-seeking)

These reveal what beginners actually want to know:

- "What is AI alignment" / "what is AI safety"
- "How to get into AI safety" / "AI safety career"
- "Is AI dangerous" / "will AI destroy humanity"
- "AI safety vs AI ethics" / "AI safety vs AI alignment"
- "AI safety jobs" / "AI safety courses"
- "Why can't we just turn off AI"
- "What is the alignment problem"
- "AI safety for beginners" / "AI safety reading list"
- "How can I help with AI safety"

### Trending and Rising Terms (2024-2026)

- "AI agents safety" (driven by agentic AI deployment)
- "AI safety testing" / "red teaming AI"
- "EU AI Act compliance"
- "AI safety institute" (government safety bodies)
- "frontier AI safety"
- "AI safety certification"

### Search Volume Patterns

Google Trends showed a sharp spike in "AI risk" and "AI safety" searches in mid-2025 (June-August), likely driven by major incidents or policy announcements. The general trend is steady growth, with spikes around major AI releases, policy events, and media coverage of AI capabilities.

---

## 2. Community Language Patterns

### A. Technical Researchers (Alignment Community)

**Where they congregate:** LessWrong, Alignment Forum, AI safety camps, MIRI, Anthropic/DeepMind safety teams

**Characteristic language:**
- "Alignment tax," "inner/outer alignment," "mesa-optimization"
- "Corrigibility," "value learning," "reward hacking"
- "Scalable oversight," "interpretability," "mechanistic interpretability"
- "Threat models," "capability overhang"
- "P(doom)" — personal probability estimates of AI catastrophe
- References to "sequences" (Yudkowsky's foundational essays)

**Tone:** Dense, precise, often assumes shared context. Heavy use of community jargon. LessWrong maintains a dedicated jargon page with terms like "affective death spiral," "anti-epistemology," and "paperclip maximizer."

**Key insight:** Even within this community, terminology is inconsistent. Multiple LessWrong posts attempt to clarify terms like "alignment" vs "x-safety," and "inner" vs "outer" alignment. Newcomers find foundational resources like "Embedded Agency" confusing.

**What they value:** Rigor, precision, intellectual honesty, novelty of research contribution.

### B. Policy and Governance People

**Where they congregate:** Think tanks (Georgetown CSET, RAND, Brookings), government AI offices, EU regulatory bodies, OECD AI forums

**Characteristic language:**
- "Risk-based framework," "high-risk AI systems," "impact assessment"
- "Regulatory sandboxes," "conformity assessment," "standards"
- "Trustworthy AI," "human oversight," "fundamental rights"
- "Dual-use," "frontier models," "compute governance"
- "Safety margins," "reporting obligations," "incident databases"
- EU AI Act terminology: "providers," "deployers," "high-risk," "unacceptable risk"

**Tone:** Formal, institutional, process-oriented. Favors frameworks and taxonomies. Avoids speculative language.

**What they value:** Implementability, precedent, jurisdictional clarity, stakeholder buy-in.

### C. Engineers and Practitioners

**Where they congregate:** ML conferences (NeurIPS safety tracks), corporate Responsible AI teams, Slack/Discord communities, GitHub

**Characteristic language:**
- "Responsible AI," "ML safety," "robustness," "fairness"
- "Guardrails," "content filtering," "red teaming"
- "Safety evaluation," "benchmark," "model card"
- "Bias mitigation," "adversarial testing," "safety fine-tuning"
- "RLHF," "constitutional AI," "safety training"

**Tone:** Practical, implementation-focused. Less philosophical, more "what do I do on Monday." Prefers concrete tools and checklists over abstract arguments.

**What they value:** Working code, reproducible results, clear guidelines, tooling.

### D. General Public

**Where they congregate:** Reddit (r/artificial, r/singularity), mainstream news comments, Twitter/X, YouTube comments

**Characteristic language:**
- "AI taking over," "robots replacing us," "AI going rogue"
- "Skynet," "HAL 9000," "the matrix" (pop culture references)
- "AI destroying jobs," "AI spying on us"
- "Is AI safe?" / "Should I be worried about AI?"
- "AI doomer" vs "AI accelerationist"
- "AGI," "superintelligence" (used loosely)

**Tone:** Emotional, narrative-driven, often binary (doomer vs. dismissive). Pop culture frames dominate. Frequent conflation of different risk types (job loss, privacy, existential risk).

**What they value:** Clear explanations, relatable analogies, honest assessment without condescension.

### E. Effective Altruism / Rationalist Adjacent

**Where they congregate:** EA Forum, 80,000 Hours, LessWrong, AI safety camps

**Characteristic language:**
- "Cause area," "neglectedness," "tractability," "importance"
- "Expected value," "counterfactual impact"
- "Career capital," "talent pipeline"
- "Field-building," "movement building"
- "Crux," "double crux," "inside view vs outside view"

**Tone:** Analytical, consequentialist, career-optimization focused. Heavy use of decision-theory concepts.

**What they value:** Impact measurement, career leverage, strategic thinking, community coordination.

### F. Corporate / Enterprise Audience

**Where they congregate:** Industry conferences (RSA, Gartner), corporate compliance teams, LinkedIn

**Characteristic language:**
- "Responsible AI framework," "AI governance," "AI risk management"
- "Compliance," "audit," "certification"
- "NIST AI RMF," "ISO standards," "SOC 2 for AI"
- "Board-level AI risk," "liability," "due diligence"
- "Safety washing" (critique from safety community)

**Tone:** Business-oriented, risk-management framing. Safety is a compliance/liability issue, not an existential concern. Emphasizes ROI of safety investment.

**What they value:** Frameworks, certifications, legal defensibility, competitive advantage.

---

## 3. Terminology Map: Same Concept, Different Words

| Concept | Technical term | Policy term | Public term | Corporate term |
|---------|---------------|-------------|-------------|----------------|
| AI does bad things | Misalignment, reward hacking | Non-compliance, high-risk | AI going rogue, AI takeover | AI incident, model failure |
| Making AI behave | Alignment, RLHF | Regulation, oversight | Safety, control | Governance, guardrails |
| Testing AI for problems | Red teaming, eval | Conformity assessment | Stress testing | Audit, certification |
| AI becomes very powerful | Capability overhang, AGI | Frontier AI, GPAI | Superintelligence, singularity | Advanced AI systems |
| Stopping bad AI | Corrigibility, shutdown | Kill switch, human override | Just turn it off | Emergency procedures |
| Long-term catastrophe | X-risk, P(doom) | Catastrophic risk, systemic risk | End of the world, Skynet | Black swan event |

---

## 4. Framing Wars: How Identity Maps to Language

The AI safety discourse is increasingly tribal, with language serving as identity markers:

### The Doomer vs. Accelerationist Axis

- **"AI doomers" / "decelerationists"**: Want to slow development. Use terms like "existential risk," "alignment crisis," "pause AI." Identity tied to concern.
- **"e/acc" (effective accelerationism)**: Want maximum speed. Use "techno-optimism," "abundance," "progress." Frame safety concerns as obstructionism.
- **"Pragmatic middle"**: Emerging group frustrated with both extremes. Use "responsible development," "safety by design," "measured approach."

### The Safety vs. Ethics Tension

- **AI Safety community**: Frames alignment as a control problem. Focuses on future catastrophic risk. Historically more white, male, EA/rationalist.
- **AI Ethics community**: Frames alignment as a justice problem. Focuses on present-day bias, fairness, labor impact. Historically more diverse, social-science oriented.
- These communities use different terminology even when discussing overlapping concerns, and rarely cite each other's work.

### What This Means for AISafety.org

The platform must be a bridge, not a tribe. Language choices signal allegiance. Using only alignment-community jargon alienates policy and ethics audiences. Using only corporate language alienates technical researchers. The platform needs a deliberate multi-register voice.

---

## 5. Key Implications for Content Strategy

1. **SEO targeting should span registers**: Content needs to rank for "AI alignment" AND "AI safety" AND "responsible AI" AND "AI governance" — these are different audiences entering through different doors.

2. **Glossary is a critical product**: The terminology confusion across communities is a genuine pain point. A well-maintained, cross-referencing glossary could be a high-value asset.

3. **Question-format content has high search intent**: "What is AI alignment" and "How to get into AI safety" are real searches with real intent. FAQ-style content performs well.

4. **Bridge language matters**: The platform should develop its own voice that is rigorous enough for technical readers, accessible enough for newcomers, and practical enough for professionals — without defaulting to any single community's jargon.

5. **Career content is high-demand**: "How to get into AI safety" is one of the most common entry-point searches. Career-oriented content has built-in audience demand.

6. **Pop culture frames are entry points, not endpoints**: Many people arrive via Terminator/Skynet framing. Good content meets them there and guides them to more accurate understanding.
