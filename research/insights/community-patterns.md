# Community Patterns and Audience Insights

> What people need, where they struggle, and what opportunities this creates for AISafety.org.

---

## 1. Common Entry Points Into AI Safety

### How People First Discover AI Safety

| Entry point | What triggers it | Typical next action |
|------------|-----------------|-------------------|
| News/media coverage | Major AI release, AI incident, policy announcement | Google "AI safety" or "AI risk" |
| Career exploration | 80,000 Hours, EA community, university course | Search for "AI safety careers" or take AGI Safety Fundamentals course |
| Technical curiosity | Working in ML, encountering safety problems firsthand | Read alignment papers, join Alignment Forum |
| Pop culture / fear | Movies, viral AI stories, doomsday headlines | Search "will AI destroy humanity," "is AI dangerous" |
| Policy/regulatory need | EU AI Act, corporate compliance mandate | Search "AI regulation," "AI governance framework" |
| Social media discourse | Twitter/X debates, doomer vs e/acc content | Browse subreddits, follow AI safety accounts |
| Academic pipeline | Professor recommendation, course reading list | Read Superintelligence, take AISES course |

### The Beginner Journey (Typical Pattern)

1. **Trigger event** (news, career advice, fear, curiosity)
2. **First search** (broad: "AI safety," "AI risk")
3. **Overwhelm** (field is vast, jargon-heavy, fragmented)
4. **Seek orientation** (reading lists, field maps, "where do I start" posts)
5. **Find community** (LessWrong, EA Forum, Reddit, Discord)
6. **Specialize or disengage** (either dive deeper or bounce off due to complexity)

**Critical insight:** Step 3 to Step 4 is where people are lost. The transition from "I'm interested" to "I understand the landscape" is poorly served. This is the exact gap AISafety.org's Field Map targets.

---

## 2. Common Beginner Questions

Sourced from AISafety.info FAQs, Reddit, Alignment Forum newcomer posts, and search data:

### Orientation Questions
- "What is AI safety / alignment / the alignment problem?"
- "Why should I care about AI safety?"
- "Is this actually a real problem or just hype?"
- "What's the difference between AI safety, AI alignment, AI ethics, and AI governance?"
- "Who are the major organizations working on this?"

### Feasibility / Skepticism Questions
- "Why can't we just turn the AI off if it misbehaves?"
- "Can't we just test an AI to make sure it's safe?"
- "Why don't we just not build AGI if it's dangerous?"
- "Isn't this just fear-mongering?"
- "How do we know AI will ever be that powerful?"

### Career Questions
- "How do I get into AI safety research?"
- "Do I need a PhD?"
- "What skills do I need?"
- "What are the main career paths (technical research, governance, field-building)?"
- "Where are the jobs?"

### Action Questions
- "What can I do to help?"
- "What should I read first?"
- "How can I convince others this is important?"
- "What are the best courses?"

---

## 3. Common Misconceptions

These recur across forums, social media, and media coverage:

| Misconception | Reality | Source of confusion |
|--------------|---------|-------------------|
| AI safety = worrying about Terminator/Skynet | Safety researchers worry about misalignment, not conscious evil robots | Pop culture dominates mental models |
| Only fringe people care about AI risk | Mainstream researchers at top labs and institutions work on safety | Media framing, dismissive tech voices |
| AI safety is only about future superintelligence | Current AI systems have real safety problems now (hallucination, bias, misuse) | Historical focus of some prominent voices |
| AI will be dangerous because it becomes conscious/evil | The concern is about misaligned goals, not consciousness | Movies and anthropomorphization |
| We're decades away, so no need to worry now | Safety research problems may take decades to solve; need to start now | Timeline uncertainty |
| More capable AI = automatically more dangerous | Capability and alignment are partially independent dimensions | Conflation of capability with risk |
| AI safety and AI ethics are the same thing | They overlap but have different intellectual traditions, focus areas, and communities | Blurred media coverage |

---

## 4. Pain Points and Unmet Needs

### What People Say They Need (Signals from Forums and Communities)

#### "The field is too fragmented"
- Dozens of organizations, no clear map of who does what
- Research scattered across papers, blog posts, forum posts, tweets
- No single trusted source for "state of the field"

#### "I can't find a good starting point"
- Reading lists exist (80,000 Hours, AISafety.info) but assume too much context
- AGI Safety Fundamentals course is good but requires significant time commitment
- Gap between "I'm curious" and "I'm taking a course"

#### "The jargon is a barrier"
- LessWrong/Alignment Forum content uses heavy community-specific terminology
- Even within the technical community, terms are used inconsistently
- No widely-adopted cross-community glossary

#### "I don't know what's important right now"
- Too many papers, blog posts, developments to track
- No reliable "what happened this week in AI safety" digest at multiple depth levels
- Existing newsletters either too technical or too surface-level

#### "Career guidance is scattered"
- 80,000 Hours is the primary source but can feel EA-specific
- AISafety.com has job listings but limited career guidance
- Talent pipeline is 2-20x larger than available positions (field growth report data)
- People want "what should someone with MY background do?"

#### "I don't know what I can do that matters"
- Common feeling of helplessness
- Desire for concrete, actionable steps beyond "read more"
- Gap between "understanding the problem" and "contributing to solutions"

#### "Resources assume I'm already technical"
- Most deep resources written for ML researchers
- Policy-oriented content is sparse compared to technical content
- Almost nothing for "I'm a manager / designer / writer who wants to contribute"

---

## 5. Professional and Enterprise Patterns

### AI Safety in the Workplace

**Emerging roles and titles:**
- AI Safety Researcher / Engineer
- AI Red Team Specialist
- Responsible AI Lead / Manager
- AI Governance Analyst
- AI Ethics Officer
- AI Safety and Compliance Officer
- ML Safety Engineer

**Skills employers look for:**
- ML/deep learning fundamentals (Python, PyTorch/TensorFlow)
- Risk assessment and threat modeling
- Red teaming and adversarial testing
- Understanding of fairness, bias, and robustness
- Policy and regulatory knowledge (especially EU AI Act)
- Cross-functional communication (translating between technical and non-technical)

**Professional development signals:**
- 85% of digital trust professionals expect to need more AI training within 2 years
- New certifications emerging: TAISE (Cloud Security Alliance), CASO (Tonex), ISACA AI certifications
- ISO/PAS 8800 (Safety and AI) driving enterprise training demand
- BlueDot Impact, CAIS, and others offering structured courses

**Enterprise demand signals:**
- "Responsible AI" framing dominates corporate discourse
- Companies need frameworks, not philosophy
- Safety as compliance/liability concern, not existential concern
- Demand for checklists, audit tools, risk assessment templates
- Gap between corporate "responsible AI principles" and actual implementation

### What Professionals Need from AISafety.org

1. **Translation layer**: Research findings explained in professional context
2. **Practical toolkits**: Assessment templates, decision frameworks, checklists
3. **Credential-adjacent learning**: Content that builds demonstrable expertise
4. **Curated current awareness**: "What changed this week that affects my work"
5. **Peer community**: Others navigating similar professional challenges

---

## 6. Community Platform Landscape

### Where AI Safety Communities Currently Live

| Platform | Audience | Strengths | Weaknesses |
|----------|----------|-----------|------------|
| LessWrong / Alignment Forum | Technical researchers, EA-adjacent | Depth, rigor, archive | Jargon barrier, intimidating to newcomers |
| EA Forum | Cause-prioritization community | Career advice, funding discussions | EA-specific framing, trust issues post-FTX |
| 80,000 Hours | Career-changers | Excellent career guidance | Narrow EA lens, limited scope |
| AISafety.info | Beginners seeking FAQs | Clear Q&A format, chatbot | Limited depth, volunteer-maintained |
| AISafety.com | Broad AI safety audience | Job board, field map, resource hub | Somewhat sparse content |
| AISafety.world | Ecosystem mappers | Visual org/people map | Limited interactivity |
| Twitter/X | Everyone, loudly | Real-time discourse | Noise, tribal dynamics, low signal |
| Reddit (r/aisafety, r/singularity) | General public, curious newcomers | Low barrier, diverse views | Variable quality, not curated |
| Discord servers | Various communities | Real-time conversation | Ephemeral, hard to search |
| Substack newsletters | Informed public | Individual voice, depth | Fragmented, no aggregation |

### The Gap

No single platform currently provides:
- Trusted orientation across all sub-communities
- Multi-register content (technical + accessible + practical)
- Curation without tribal allegiance
- Career guidance that isn't EA-specific
- Professional tools alongside educational content
- A bridge between technical researchers and everyone else

This is the positioning opportunity for AISafety.org.

---

## 7. Strategic Implications

### Content Voice Recommendations

1. **Intellectually honest, not tribal**: Acknowledge uncertainty and disagreement without picking sides in doomer/e-acc debates
2. **Multi-register**: Write at multiple levels of depth, clearly labeled (Beginner / Intermediate / Advanced)
3. **Bridge language**: Develop a house style that is precise without being jargon-heavy, accessible without being simplistic
4. **Question-first**: Lead with the questions people actually ask, not the answers researchers want to give
5. **Practical over philosophical**: Ground abstract concepts in concrete examples and real-world relevance

### Product Opportunities (Ranked by Demand Signal Strength)

| Opportunity | Demand signal | Difficulty |
|------------|---------------|------------|
| Interactive field map / orientation | Very strong — most common pain point | Medium |
| Cross-community glossary | Strong — terminology confusion is universal | Low |
| Curated weekly digest at multiple depths | Strong — information overload is universal | Medium (ongoing) |
| Career pathways by background | Strong — career questions are top searches | Medium |
| Practical toolkit for professionals | Strong — enterprise demand growing fast | Medium-High |
| Beginner-friendly explainer series | Moderate-strong — many alternatives exist but gaps remain | Low-Medium |
| Community directory with context | Moderate — people want to find their community | Low |

### SEO Priority Terms

**Tier 1 (highest volume, broadest audience):**
- AI safety, AI risk, AI alignment, AI ethics

**Tier 2 (high intent, career/orientation):**
- AI safety careers, how to get into AI safety, AI safety courses
- What is AI alignment, AI safety reading list, AI safety organizations

**Tier 3 (professional/enterprise):**
- Responsible AI framework, AI governance, AI risk management
- EU AI Act compliance, AI safety certification, AI red teaming

**Tier 4 (technical/niche but valuable):**
- AI alignment research, mechanistic interpretability, RLHF safety
- Scalable oversight, AI safety benchmarks
