# AI Safety Ecosystem Map

> Living document. Last updated: 2026-02-23. This map is under active development.

## Subfields

### Technical AI Safety
- **Alignment** — ensuring AI systems pursue intended goals
- **Interpretability/Explainability** — understanding what models do and why
- **Robustness** — making systems reliable under distribution shift
- **Evaluation & Benchmarking** — measuring safety properties
- **Red teaming** — adversarial testing of AI systems
- **Constitutional AI / RLHF** — training methods for alignment

### AI Governance
- **Regulation & Policy** — government oversight frameworks
- **Standards & Certification** — industry standards (ISO, NIST)
- **International Coordination** — multilateral AI governance
- **Corporate Governance** — internal safety practices at labs

### AI Ethics
- **Fairness & Bias** — discrimination in AI systems
- **Privacy** — data protection and surveillance
- **Transparency** — disclosure and accountability
- **Social Impact** — labor, inequality, power concentration
- **Environmental Impact** — compute and energy costs

### Existential Risk / Long-term Safety
- **Superintelligence** — risks from highly capable AI
- **Control Problem** — maintaining meaningful human oversight
- **Coordination** — preventing races to unsafe deployment
- **Forecasting** — predicting AI capability trajectories

## Key Organizations

_To be populated by field mapping research team._

### Categories:
- Research labs with safety teams
- Dedicated safety research orgs
- Policy and governance orgs
- Academic centers
- Civil society / advocacy
- Government bodies

## Active Debates

_To be populated by research team._

## Entry Points by Role

_To be populated based on persona research._
