# AI Safety Organizations: Field Map

*Last updated: February 2026*

This document catalogs major organizations working on AI safety across research, policy, governance, and advocacy. Organizations are grouped by primary function, though many span multiple categories.

---

## 1. Dedicated AI Safety Research Organizations

### Machine Intelligence Research Institute (MIRI)
- **URL:** https://intelligence.org
- **Focus:** Alignment research, mathematical foundations of AI safety
- **Description:** Founded in 2000 (as the Singularity Institute), MIRI is one of the oldest organizations focused on long-term AI safety. It conducts technical research on the mathematics of aligning smarter-than-human AI systems with human values, with a focus on agent foundations and decision theory.
- **Key outputs:** Research on logical uncertainty, Embedded Agency sequence, decision theory papers
- **Funding model:** Nonprofit, funded primarily by individual donors and grants

### Alignment Research Center (ARC)
- **URL:** https://alignment.org
- **Focus:** Theoretical alignment research, mechanistic explanations of neural network behavior
- **Description:** Founded by Paul Christiano, ARC pursues theoretical research on producing formal mechanistic explanations of neural network behavior. ARC has reported accelerating conceptual and theoretical progress, with 2025 being their fastest period of advancement since 2022.
- **Key outputs:** Eliciting Latent Knowledge (ELK) research agenda, theoretical alignment frameworks
- **Funding model:** Nonprofit, supported by Open Philanthropy and other EA-aligned funders

### METR (Model Evaluation and Threat Research)
- **URL:** https://metr.org
- **Focus:** AI capability and risk evaluations
- **Description:** Spun out of ARC Evals (originally founded by Beth Barnes within ARC) in December 2023 as an independent nonprofit. METR conducts research and evaluations to improve public understanding of the capabilities and risks of frontier AI systems, and advises AI developers and governments on implementing risk assessment methodologies.
- **Key outputs:** Frontier model evaluations, risk assessment frameworks, Frontier AI Safety Policy guidance
- **Funding model:** Nonprofit

### Center for AI Safety (CAIS)
- **URL:** https://safe.ai
- **Focus:** Technical AI safety research, AI risk communication
- **Description:** Founded in 2022 by Dan Hendrycks and Oliver Zhang, CAIS is a San Francisco-based nonprofit promoting the safe development and deployment of AI. It encompasses research in both technical AI safety and AI ethics, and gained widespread attention for its "Statement on AI Risk" signed by hundreds of researchers.
- **Key outputs:** Statement on AI Risk, AI risk taxonomy, safety benchmarks, technical research papers
- **Funding model:** Nonprofit, supported by Open Philanthropy

### Redwood Research
- **URL:** https://redwoodresearch.org
- **Focus:** Threat assessment and mitigation for AI systems
- **Description:** Redwood Research is a small, focused lab pioneering research into threat assessment and mitigation for AI systems. The team of approximately 10 researchers works on identifying and addressing potential failure modes in advanced AI.
- **Key outputs:** Adversarial training research, interpretability work, causal scrubbing
- **Funding model:** Nonprofit, funded by Open Philanthropy

### Conjecture
- **URL:** https://conjecture.dev
- **Focus:** Interpretability, conceptual alignment, epistemology
- **Description:** Conjecture is an AI safety company working on interpretability and conceptual alignment research. Their initial research agenda focused on interpretability, conceptual alignment, and epistemology. They have raised at least $10M in funding.
- **Key outputs:** Research on interpretability methods, alignment theory
- **Funding model:** For-profit company with venture funding

### Apollo Research
- **URL:** https://apolloresearch.ai
- **Focus:** AI deception detection, behavioral evaluations, interpretability
- **Description:** Apollo Research focuses on reducing dangerous capabilities in advanced AI systems, especially deceptive behaviors. They design model evaluations and conduct interpretability research, with particular concern about deceptive alignment as a potential catastrophic risk vector.
- **Key outputs:** Research on AI scheming behavior, model evaluation frameworks, interpretability research
- **Funding model:** Nonprofit (fiscally sponsored by Rethink Priorities, a registered 501(c)(3))

### FAR.AI
- **URL:** https://far.ai
- **Focus:** AI safety research, adversarial robustness
- **Description:** FAR.AI conducts safety research with a focus on adversarial robustness and identifying vulnerabilities in frontier AI systems. In 2025, their research demonstrated that leading frontier models have guardrails that can be easily removed while preserving response quality.
- **Key outputs:** Research on guardrail robustness, fine-tuning vulnerability studies
- **Funding model:** Nonprofit, supported by Open Philanthropy

### Epoch AI
- **URL:** https://epoch.ai
- **Focus:** AI development trends, compute forecasting, data analysis
- **Description:** Epoch AI is a multidisciplinary research institute investigating the trajectory and impact of artificial intelligence. They publish datasets, data visualizations, research reports, and predictive models analyzing the forces shaping AI development, supporting AI strategy and forecasting around transformative AI.
- **Key outputs:** Training compute trend datasets, scaling forecasts, AI development trajectory reports, impact reports
- **Funding model:** Nonprofit research institute

### SaferAI
- **URL:** https://safer-ai.org
- **Focus:** AI risk management, governance research, safety benchmarking
- **Description:** SaferAI is a governance and research nonprofit focused on AI risk management. They work to incentivize responsible AI practices through policy recommendations, research, and risk assessment tools. They conduct the analysis behind the Future of Life Institute's AI Safety Index.
- **Key outputs:** AI Safety Index methodology, policy recommendations, risk assessment tools
- **Funding model:** Nonprofit

---

## 2. AI Labs with Safety Teams

### Anthropic
- **URL:** https://anthropic.com
- **Focus:** AI safety research, alignment science, responsible scaling
- **Description:** Founded in 2021 by former OpenAI researchers including Dario and Daniela Amodei, Anthropic is an AI safety company that develops frontier AI systems (Claude) while conducting extensive safety research. Their Alignment Science team works on interpretability, alignment evaluations, and responsible scaling policies.
- **Key outputs:** Constitutional AI, Responsible Scaling Policy, alignment evaluation frameworks, cross-company safety evaluations (joint pilot with OpenAI in 2025), interpretability research
- **Funding model:** For-profit (public benefit corporation), venture-funded

### Google DeepMind
- **URL:** https://deepmind.google
- **Focus:** AI safety, alignment, interpretability, frontier safety
- **Description:** Google DeepMind maintains dedicated safety teams focused on existential risk, with three main research bets: amplified oversight for alignment signals, frontier safety to assess catastrophic risk capabilities, and mechanistic interpretability. The team has acknowledged ongoing revisions to their high-level approach to technical AGI safety.
- **Key outputs:** Mechanistic interpretability research, frontier model safety evaluations, safety frameworks
- **Funding model:** Corporate (subsidiary of Alphabet/Google)

### OpenAI
- **URL:** https://openai.com
- **Focus:** AI safety, alignment research, preparedness
- **Description:** OpenAI develops frontier AI systems (GPT series, o-series reasoning models) and maintains safety and alignment teams. In 2025, OpenAI participated in a joint alignment evaluation exercise with Anthropic, testing models for sycophancy, self-preservation, and misuse propensities.
- **Key outputs:** Preparedness Framework, safety evaluations, alignment research, joint industry safety exercises
- **Funding model:** Capped-profit (transitioned from nonprofit origins)

### Meta AI (FAIR)
- **URL:** https://ai.meta.com
- **Focus:** Open AI research, responsible AI development
- **Description:** Meta's Fundamental AI Research lab (FAIR) develops open-source AI models (Llama series) and conducts research on responsible AI development. Their approach emphasizes open-source safety research and community-driven safety improvements.
- **Key outputs:** Llama models, responsible use guides, open-source safety tooling
- **Funding model:** Corporate (division of Meta)

### xAI
- **URL:** https://x.ai
- **Focus:** AI development, understanding AI systems
- **Description:** Founded by Elon Musk in 2023, xAI develops large language models (Grok series) with a stated mission of understanding the true nature of the universe. The company has taken a distinct approach to AI safety compared to other major labs.
- **Key outputs:** Grok models
- **Funding model:** For-profit, venture-funded

---

## 3. Policy and Governance Organizations

### Centre for the Governance of AI (GovAI)
- **URL:** https://governance.ai
- **Focus:** AI governance research, policy talent development
- **Description:** GovAI produces rigorous research and fosters talent to help decision-makers navigate the transition to a world with advanced AI. GovAI alumni have gone on to influential roles in governments across the US, UK, and EU, major AI companies (DeepMind, OpenAI, Anthropic), and prominent think tanks (RAND, CSET).
- **Key outputs:** Governance research papers, Summer Fellowship program, policy briefs
- **Funding model:** Nonprofit, affiliated with University of Oxford

### Center for Security and Emerging Technology (CSET)
- **URL:** https://cset.georgetown.edu
- **Focus:** AI governance, national security implications of emerging technology
- **Description:** Based at Georgetown University, CSET studies the security implications of emerging technologies, with a major focus on AI governance. In 2025, they published work examining how regulators can govern AI effectively and whether existing federal authorities suffice or new legal powers are needed.
- **Key outputs:** Policy briefs, data-driven analysis, research on AI governance frameworks, workforce studies
- **Funding model:** University-based research center, funded by Open Philanthropy

### AI Now Institute
- **URL:** https://ainowinstitute.org
- **Focus:** Social implications of AI, corporate power, AI accountability
- **Description:** Founded at New York University, AI Now researches the social implications of artificial intelligence, with emphasis on rights, liberties, and corporate accountability. They focus on the concentration of power in the AI industry and its effects on society.
- **Key outputs:** Annual AI Now Reports, policy research, advocacy on algorithmic accountability
- **Funding model:** Nonprofit, university-affiliated

### RAND Corporation (AI Policy)
- **URL:** https://rand.org
- **Focus:** AI policy research, national security, socioeconomic impacts
- **Description:** RAND's Social and Economic Policy Rethink Initiative provides research on the benefits, challenges, and policy implications of widespread AI adoption. Their AI-related work spans risk assessment for AI loss-of-control incidents, emergency response protocols, and comprehensive policy analysis.
- **Key outputs:** Policy reports on AI risks, loss-of-control incident analysis, socioeconomic impact studies
- **Funding model:** Nonprofit research organization (think tank)

### Carnegie Endowment for International Peace (AI Program)
- **URL:** https://carnegieendowment.org
- **Focus:** International AI governance, AI safety as global public good
- **Description:** Carnegie's technology program examines AI safety through an international lens, studying AI safety as a global public good and analyzing the future of international scientific assessments of AI risk. They provide research informing multilateral governance efforts.
- **Key outputs:** Research on international AI governance, global public good frameworks
- **Funding model:** Nonprofit endowment

### Rethink Priorities
- **URL:** https://rethinkpriorities.org
- **Focus:** AI strategy, catastrophic risk research, cause prioritization
- **Description:** Rethink Priorities conducts decision-oriented research to reduce global catastrophic risks, including from AI. In 2026, they are launching a dedicated AI Strategy team focused on generating insights to inform critical policy and strategic choices around transformative AI.
- **Key outputs:** AI governance research, cause prioritization analysis, decision-relevant strategy research
- **Funding model:** Nonprofit (501(c)(3))

---

## 4. Academic Centers

### Center for Human-Compatible Artificial Intelligence (CHAI) - UC Berkeley
- **URL:** https://humancompatible.ai
- **Focus:** Human-compatible AI, value alignment, cooperative AI
- **Description:** Founded in 2016 by Stuart Russell and colleagues, CHAI is a multi-university research center headquartered at UC Berkeley focusing on ensuring AI systems are beneficial to humans. Faculty span Berkeley, Cornell, University of Michigan, and Princeton, working on alignment, cooperative inverse reinforcement learning, and human-AI interaction.
- **Key outputs:** Research on inverse reinforcement learning, cooperative AI, Stuart Russell's "Human Compatible" framework
- **Funding model:** University research center, funded by Open Philanthropy, industry partners

### Future of Humanity Institute (FHI) - Oxford (Closed 2024)
- **URL:** https://fhi.ox.ac.uk (archived)
- **Focus:** Existential risk, AI safety, long-term future (historical)
- **Description:** Founded by Nick Bostrom in 2005 at the Oxford Martin School, FHI was a pioneering interdisciplinary research center studying existential risks including those from advanced AI. The University of Oxford closed FHI on April 16, 2024 after years of administrative conflicts, including a fundraising and hiring freeze imposed by the Faculty of Philosophy starting in 2020.
- **Key outputs:** Nick Bostrom's "Superintelligence," foundational existential risk research, training generations of AI safety researchers
- **Funding model:** Was university-based, funded by Amlin, Elon Musk, European Research Council, FLI, Leverhulme Trust

### Cambridge AI Safety Hub (CAISH)
- **URL:** https://caish.org
- **Focus:** AI existential risk reduction, community building
- **Description:** CAISH brings together students and professionals at Cambridge to reduce existential risks from advanced AI systems. It serves as a hub for AI safety research, education, and community-building at one of the world's leading universities.
- **Key outputs:** Research community building, workshops, educational programs
- **Funding model:** University-affiliated

### Centre for the Study of Existential Risk (CSER) - University of Cambridge
- **URL:** https://cser.ac.uk
- **Focus:** Existential risk research, including AI
- **Description:** Based at the University of Cambridge, CSER is a multidisciplinary research center studying existential risks to civilization, with AI as a major research strand. The center brings together scientists, philosophers, and policy researchers to understand and mitigate catastrophic risks.
- **Key outputs:** Research papers on existential risk, policy briefs, interdisciplinary risk frameworks
- **Funding model:** University research center

### Stanford Institute for Human-Centered Artificial Intelligence (HAI)
- **URL:** https://hai.stanford.edu
- **Focus:** Human-centered AI research, AI policy, AI Index
- **Description:** Stanford HAI conducts interdisciplinary research on AI with a human-centered focus, bringing together researchers across Stanford's schools. They produce influential benchmarking and trend analysis and convene policy discussions on AI governance.
- **Key outputs:** Annual AI Index Report, policy research, interdisciplinary AI research
- **Funding model:** University institute, industry and philanthropic funding

---

## 5. Government Bodies

### UK AI Security Institute (AISI, formerly AI Safety Institute)
- **URL:** https://aisi.gov.uk
- **Focus:** AI model evaluations, frontier safety, national security
- **Description:** Established within the UK Department of Science, Innovation, and Technology, the institute was renamed from "AI Safety Institute" to "AI Security Institute" in February 2025, signaling a shift toward security-focused AI risk assessment. It has tested 16 models including at least three frontier models ahead of public launch, and evaluated 22 anonymized models with 1.8 million attempts to break safeguards.
- **Key outputs:** Frontier model safety evaluations, pre-deployment testing, research publications
- **Funding model:** Government (UK DSIT)

### US Center for AI Standards and Innovation (CAISI, formerly US AISI)
- **URL:** https://nist.gov/caisi
- **Focus:** AI standards, innovation support, safety evaluation
- **Description:** Housed within NIST, the US AI Safety Institute was renamed to CAISI in June 2025. The mission shifted from safety research toward evaluating and enhancing US innovation in commercial AI systems while ensuring national security standards, reflecting the change in US administration priorities.
- **Key outputs:** AI standards development, safety evaluation frameworks, AI Safety Institute Consortium (AISIC) coordination
- **Funding model:** Government (US Department of Commerce / NIST)

### AI Safety Institute Consortium (AISIC, US)
- **URL:** https://nist.gov (part of NIST)
- **Focus:** Multi-stakeholder AI safety collaboration
- **Description:** AISIC brings together over 280 organizations from industry, academia, government, and civil society to work on AI safety problems. It serves as a coordinating body for collaborative safety research and standards development.
- **Key outputs:** Collaborative safety research, consensus standards
- **Funding model:** Government-convened consortium

### EU AI Office
- **URL:** https://digital-strategy.ec.europa.eu/en/policies/ai-office
- **Focus:** EU AI Act implementation, AI governance
- **Description:** The EU AI Office oversees implementation and enforcement of the EU AI Act, the world's first comprehensive AI regulation. It works to ensure AI systems placed on the European market meet safety and fundamental rights requirements.
- **Key outputs:** EU AI Act enforcement, regulatory guidance, codes of practice for general-purpose AI
- **Funding model:** Government (European Commission)

### Japan AI Safety Institute (JAISI)
- **URL:** https://aisi.go.jp
- **Focus:** AI safety evaluation, standards development
- **Description:** Established as part of the international network of AI Safety Institutes agreed at the 2024 AI Seoul Summit. JAISI contributes to international collaboration on AI safety evaluation and standards.
- **Key outputs:** Safety evaluation participation, international collaboration
- **Funding model:** Government (Japan)

### Singapore AI Safety Institute
- **Focus:** AI safety testing, standards
- **Description:** Part of the growing international network of AI Safety Institutes, Singapore's institute focuses on AI safety testing and developing standards appropriate for the Asia-Pacific context. Singapore was among the first wave of countries to establish a dedicated AI safety body.
- **Key outputs:** Safety testing frameworks, regional collaboration
- **Funding model:** Government (Singapore)

### India AI Safety Institute
- **Focus:** Ethical AI development, AI safety for Indian context
- **Description:** Announced in January 2025 by India's Minister for Electronics & IT, the institute aims to ensure the ethical and safe application of AI models, promoting domestic R&D grounded in India's social, economic, cultural, and linguistic diversity.
- **Key outputs:** Newly established; focused on safety standards for Indian AI development
- **Funding model:** Government (India)

### International Network of AI Safety Institutes
- **Focus:** International coordination on AI safety
- **Description:** Agreed upon at the AI Seoul Summit in May 2024, this network comprises AI Safety Institutes from the UK, US, Japan, France, Germany, Italy, Singapore, South Korea, Australia, Canada, and the EU. It coordinates international efforts on frontier AI safety evaluation and standards.
- **Key outputs:** Second International AI Safety Report (February 2026, led by Yoshua Bengio, authored by 100+ experts from 30+ countries)
- **Funding model:** Multi-government coordination

---

## 6. Civil Society, Advocacy, and Field-Building Organizations

### Future of Life Institute (FLI)
- **URL:** https://futureoflife.org
- **Focus:** Existential risk mitigation, AI governance advocacy
- **Description:** FLI works to reduce large-scale risks from transformative technologies, with AI as a primary focus. They publish the AI Safety Index (analysis conducted by SaferAI), advocate for AI regulation, and fund research on existential risk mitigation.
- **Key outputs:** AI Safety Index (Winter/Summer editions), "Pause Giant AI Experiments" open letter, policy recommendations, grant programs
- **Funding model:** Nonprofit, donations (historically significant funding from Jaan Tallinn)

### Partnership on AI (PAI)
- **URL:** https://partnershiponai.org
- **Focus:** Responsible AI development, multi-stakeholder collaboration
- **Description:** PAI is an independent nonprofit originally established by a coalition of major tech companies (Apple, Amazon, Meta, Google/DeepMind, IBM, Microsoft), civil society organizations, and academic institutions. In 2025, they broadened their global community and expanded expertise in emerging technology governance.
- **Key outputs:** AI governance priority frameworks (Six AI Governance Priorities for 2026), best practices, multi-stakeholder guidelines
- **Funding model:** Nonprofit, funded by corporate partners and grants

### Americans for Responsible Innovation (ARI)
- **URL:** https://ari.us
- **Focus:** AI policy advocacy, legislative engagement
- **Description:** ARI is a nonprofit organization dedicated to policy advocacy in the public interest around emerging technologies, particularly AI. They coordinate coalitions for legislative advocacy, including mobilizing over 60,000 Americans in 2025 to urge the US Senate to protect state AI safety enforcement rights.
- **Key outputs:** Legislative advocacy, coalition letters (e.g., NIST funding support with 90+ organizations), policy analysis
- **Funding model:** Nonprofit

### Encode Justice (Encode)
- **URL:** https://encodeai.org
- **Focus:** Youth-led AI ethics advocacy, legislative campaigns
- **Description:** Encode is a global youth-led organization advocating for ethical AI regulation, founded by Sneha Revanur. It has mobilized thousands of young people to address algorithmic bias and AI accountability, achieving significant legislative impact including sponsoring California SB 53, a landmark US AI safety law.
- **Key outputs:** California SB 53 (Transparency in Frontier AI Act), first US law governing AI in nuclear weapons, grassroots advocacy campaigns
- **Funding model:** Nonprofit

### 80,000 Hours
- **URL:** https://80000hours.org
- **Focus:** Career guidance for high-impact work, including AI safety
- **Description:** 80,000 Hours provides evidence-based career advice for people wanting to have a positive impact, with AI safety as a top priority area. They survey AI safety organizations on hiring needs and provide extensive resources for entering the field.
- **Key outputs:** AI safety career reviews, hiring surveys from 38 top AI safety organizations, technical upskilling resource lists, AI risk reading lists
- **Funding model:** Nonprofit, part of the Centre for Effective Altruism

### Open Philanthropy
- **URL:** https://openphilanthropy.org
- **Focus:** AI safety grantmaking, cause prioritization
- **Description:** Open Philanthropy is a major funder of the AI safety ecosystem, having deployed hundreds of millions of dollars toward AI safety research since 2014. In 2025, they launched a $40M Request for Proposals for technical AI safety research across 21 research areas.
- **Key outputs:** Grantmaking across the AI safety field, $40M technical AI safety RFP, funding for ARC, CAIS, GovAI, FAR AI, Redwood Research, and many others
- **Funding model:** Philanthropic foundation (funded primarily by Dustin Moskovitz and Cari Tuna)

### EleutherAI
- **URL:** https://eleuther.ai
- **Focus:** Open-source AI research, interpretability, AI safety
- **Description:** EleutherAI is a grassroots research collective that has carved out a niche researching how AI systems make decisions, maintaining widely-used training datasets, and shaping global policy around AI safety and transparency. Backed by grants from Open Philanthropy, Omidyar Network, Hugging Face, and Stability AI.
- **Key outputs:** Open-source language models, training datasets, interpretability research, policy contributions
- **Funding model:** Nonprofit, grants from philanthropies and industry

### All Tech Is Human
- **URL:** https://alltechishuman.org
- **Focus:** Responsible tech ecosystem, AI safety community building
- **Description:** All Tech Is Human works to build the responsible tech pipeline and ecosystem, including publishing guides on the global landscape of AI Safety Institutes and connecting stakeholders across the AI safety community.
- **Key outputs:** Global AI Safety Institute landscape reports, community building, responsible tech guides
- **Funding model:** Nonprofit

---

## Summary Statistics

| Category | Count |
|----------|-------|
| Dedicated AI safety research orgs | 10 |
| AI labs with safety teams | 5 |
| Policy and governance orgs | 7 |
| Academic centers | 5 |
| Government bodies | 8 |
| Civil society / advocacy / field-building | 9 |
| **Total** | **44** |

---

## Notes

- The AI safety field has approximately 600 FTEs working on technical AI safety and 500 FTEs on non-technical AI safety (1,100 total) as of 2025, spread across roughly 70 organizations.
- Several organizations have undergone significant name changes or closures: FHI (Oxford) closed in April 2024; UK AISI renamed to AI Security Institute in February 2025; US AISI renamed to CAISI in June 2025.
- The field saw the formation of an international network of AI Safety Institutes following the 2024 AI Seoul Summit, with 7 new national institutes established since.
- The second International AI Safety Report was published in February 2026, representing the largest global collaboration on AI safety to date.
- Open Philanthropy remains the single largest funder in the AI safety ecosystem.
